<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"monsoon-cs.moe","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.21.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="IntroductionQuantization is a commonly used acceleration technique in NN inference. The primary computational workloads in NNs come from Convolution, Linear Layers, and Attention, which are implemente">
<meta property="og:type" content="article">
<meta property="og:title" content="How Quantization Works: From a Matrix Multiplication Perspective">
<meta property="og:url" content="https://monsoon-cs.moe/2024-03-06-quantization-gemm/">
<meta property="og:site_name" content="Monsoon&#39;s Blog">
<meta property="og:description" content="IntroductionQuantization is a commonly used acceleration technique in NN inference. The primary computational workloads in NNs come from Convolution, Linear Layers, and Attention, which are implemente">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://monsoon-cs.moe/2024-03-06-quantization-gemm/quant_matrix.png">
<meta property="og:image" content="https://monsoon-cs.moe/2024-03-06-quantization-gemm/llm_int8.png">
<meta property="og:image" content="https://monsoon-cs.moe/2024-03-06-quantization-gemm/smooth_quant_motivation.png">
<meta property="og:image" content="https://monsoon-cs.moe/2024-03-06-quantization-gemm/smooth_quant.png">
<meta property="og:image" content="https://monsoon-cs.moe/2024-03-06-quantization-gemm/smooth_quant_2.png">
<meta property="og:image" content="https://monsoon-cs.moe/2024-03-06-quantization-gemm/zero_quant.png">
<meta property="article:published_time" content="2024-03-05T16:00:00.000Z">
<meta property="article:modified_time" content="2024-03-06T16:00:00.000Z">
<meta property="article:author" content="Monsoon">
<meta property="article:tag" content="ml-system">
<meta property="article:tag" content="llm">
<meta property="article:tag" content="quantization">
<meta property="article:tag" content="gemm">
<meta property="article:tag" content="cuda-kernel">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://monsoon-cs.moe/2024-03-06-quantization-gemm/quant_matrix.png">


<link rel="canonical" href="https://monsoon-cs.moe/2024-03-06-quantization-gemm/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://monsoon-cs.moe/2024-03-06-quantization-gemm/","path":"2024-03-06-quantization-gemm/","title":"How Quantization Works: From a Matrix Multiplication Perspective"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>How Quantization Works: From a Matrix Multiplication Perspective | Monsoon's Blog</title>
  



  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{&quot;token&quot;: &quot;3c9e3103682b4f6fbc36342486e67640&quot;}'></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Monsoon's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Monsoon's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Let%E2%80%99s-do-some-math"><span class="nav-number">2.</span> <span class="nav-text">Let’s do some math</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Some-basic-quantization-methods"><span class="nav-number">3.</span> <span class="nav-text">Some basic quantization methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Per-element-and-Per-channel"><span class="nav-number">3.1.</span> <span class="nav-text">Per-element and Per-channel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Per-token-and-per-tensor"><span class="nav-number">3.2.</span> <span class="nav-text">Per-token and per-tensor</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hardware-requirements"><span class="nav-number">4.</span> <span class="nav-text">Hardware requirements</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Performance-analysis"><span class="nav-number">5.</span> <span class="nav-text">Performance analysis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Some-LLM-Quantization-works"><span class="nav-number">6.</span> <span class="nav-text">Some LLM Quantization works</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LLM-int8"><span class="nav-number">6.1.</span> <span class="nav-text">LLM.int8()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SmoothQuant"><span class="nav-number">6.2.</span> <span class="nav-text">SmoothQuant</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ZeroQuant"><span class="nav-number">6.3.</span> <span class="nav-text">ZeroQuant</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">7.</span> <span class="nav-text">Conclusion</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Monsoon</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">52</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/monsoon235" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;monsoon235" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://t.me/monsoon235" title="Telegram → https:&#x2F;&#x2F;t.me&#x2F;monsoon235" rel="noopener me" target="_blank"><i class="fab fa-telegram fa-fw"></i>Telegram</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/big/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://monsoon-cs.moe/2024-03-06-quantization-gemm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Monsoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="How Quantization Works: From a Matrix Multiplication Perspective | Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          How Quantization Works: From a Matrix Multiplication Perspective
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-03-06 00:00:00" itemprop="dateCreated datePublished" datetime="2024-03-06T00:00:00+08:00">2024-03-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-07 00:00:00" itemprop="dateModified" datetime="2024-03-07T00:00:00+08:00">2024-03-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Quantization is a commonly used acceleration technique in NN inference. The primary computational workloads in NNs come from Convolution, Linear Layers, and Attention, which are implemented by GEMM in the lower level. This blog aims to <strong>discuss the principles of quantization from the matrix multiplication perspective and to explain why some quantization methods are impractical</strong>. It also aims to review several LLM quantization methods from this perspective.</p>
<p>I define <strong>practical quantization</strong> as follows:</p>
<ol>
<li>Operation <strong>can still be performed using GEMM after quantization</strong>. This requires both mathematical feasibility and hardware support. It is a fundamental requirement for achieving acceleration.</li>
<li>Quantization must lead to <strong>actual acceleration</strong>. Acceleration can arise from higher INT8 hardware throughput, or from the memory bandwidth saved by smaller memory footprint. Importantly, the benefits of acceleration must outweigh the quantization overhead.</li>
</ol>
<h2 id="Let’s-do-some-math"><a href="#Let’s-do-some-math" class="headerlink" title="Let’s do some math"></a>Let’s do some math</h2><p>Suppose an operator can be expressed in the form of matrix multiplication:<br>$$\mathbf{Y}&#x3D;\mathbf{X} \mathbf{W}^\top,$$<br>where $\mathbf{X} \in \mathbb{R}^{N \times C}$, $\mathbf{Y} \in \mathbb{R}^{N \times D}$, $\mathbf{W} \in \mathbb{R}^{D \times C}$, while their quantized versions are denoted as $\hat{\mathbf{X}}$, $\hat{\mathbf{Y}}$, $\hat{\mathbf{W}}$. Our goal is to ensure that operations can still be performed using GEMM after quantization, i.e.:<br>$$\hat{\mathbf{Y}}&#x3D;\hat{\mathbf{X}} \hat{\mathbf{W}}^\top.$$</p>
<p>Let the <strong>per-element</strong> quantization functions for $\mathbf{X}$, $\mathbf{Y}$, and $\mathbf{W}$ be denoted as $p_{nc}(\cdot)$, $q_{nd}(\cdot)$, $r_{dc}(\cdot)$ respectively:<br>$$\begin{aligned}<br>    \hat{x}_ {nc} &amp;&#x3D; p_ {nc}(x_{nc}), \\<br>    \hat{y}_ {nd} &amp;&#x3D; q_ {nd}(y_{nd}), \\<br>    \hat{w}_ {dc} &amp;&#x3D; r_ {dc}(w_{dc}).<br>\end{aligned}$$<br>The corresponding dequantization functions are denoted as $p_ {nc}^{-1}(\cdot)$, $q_ {nd}^{-1}(\cdot)$, $r_ {dc}^{-1}(\cdot)$, i.e.:<br>$$\begin{aligned}<br>y_ {nd}<br>&amp;&#x3D; \sum_ {c&#x3D;1}^{C} x_ {nc} w_ {dc}, \\<br>q_ {nd}^{-1}(\hat{y}_ {nd}) &amp;&#x3D; \sum_ {c&#x3D;1}^{C} p_ {nc}^{-1}(\hat{x}_ {nc}) \cdot r_ {dc}^{-1}(\hat{w}_ {dc}).<br>\end{aligned}$$<br>The above formulas set the <strong>basic constraints</strong> that <strong>practical quantization</strong> should satisfy mathematically.</p>
<h2 id="Some-basic-quantization-methods"><a href="#Some-basic-quantization-methods" class="headerlink" title="Some basic quantization methods"></a>Some basic quantization methods</h2><p>With this basic constraints, we can now discuss several fundamental quantization methods, including per-element, per-channel, per-token, and per-tensor quantization.</p>
<h3 id="Per-element-and-Per-channel"><a href="#Per-element-and-Per-channel" class="headerlink" title="Per-element and Per-channel"></a>Per-element and Per-channel</h3><p>In the basic constraints mentioned above, the dequantization function $q_ {nd}^{-1}(\cdot)$ on the left-hand side does not depend on $c$. Clearly, if the right-hand side quantization functions $p_ {nc}^{-1}(\cdot)$ and $r_ {dc}^{-1}(\cdot)$ depend on $c$, <strong>this constraint will be violated</strong>. This implies that these two conditions cannot be satisfied at the same time:</p>
<ol>
<li>Computation can be done by GEMM.</li>
<li>Different quantization functions can be applied in different channels of $\mathbf{X}$ and $\mathbf{W}$.</li>
</ol>
<p>In other words, this indicates that <strong>per-element and per-channel quantization cannot be accelerated using GEMM. They are impractical</strong>.</p>
<h3 id="Per-token-and-per-tensor"><a href="#Per-token-and-per-tensor" class="headerlink" title="Per-token and per-tensor"></a>Per-token and per-tensor</h3><p>From the above discussion, we know that practical quantization needs to satisfy at least:<br>$$\begin{aligned}<br>    p_ {n}(\cdot) &amp;&#x3D; p_ {nc} (\cdot), \quad \forall n, c, \\<br>    r_ {d}(\cdot) &amp;&#x3D; r_ {dc} (\cdot), \quad \forall d, c.<br>\end{aligned}$$<br>That is, the quantization function is same for all channels. Therefore, the basic constraint can be formulated as:<br>$$q_ {nd}^{-1}(\hat{y}_ {nd}) &#x3D; \sum_ {c&#x3D;1}^{C_ i} p_ {n}^{-1}(\hat{x}_ {nc}) \cdot r_ {d}^{-1}(\hat{w}_ {dc}),$$<br>Thus, we get <strong>per-channel quantization</strong>. If we further assume:<br>$$\begin{aligned}<br>    p(\cdot) &amp;&#x3D; p_ {nc} (\cdot), \quad \forall n, c, \\<br>    r(\cdot) &amp;&#x3D; r_ {dc} (\cdot), \quad \forall d, c.<br>\end{aligned}$$<br>That is, the quantization function is same for all elements in both $\mathbf{X}$ and $\mathbf{W}$. Therefore, the basic constraint can be formulated as:<br>$$q_ {nd}^{-1}(\hat{y}_ {nd}) &#x3D; q^{-1}(\hat{y}_ {nd}) &#x3D; \sum_ {c&#x3D;1}^{C_i} p^{-1}(\hat{x}_ {nc}) \cdot r^{-1}(\hat{w}_ {dc}).$$<br>We thus obtain <strong>per-tensor quantization</strong>. While both of these quantization methods have theoretical feasibility, the practical values of them are still limited by hardware support (as discussed in the next section).</p>
<p>For convenience, the following discussion focuses only on per-token quantization. Per-tensor quantization can be seen as a special case of per-token quantization. The most commonly used quantization method in practice is <strong>symmetric uniform quantization</strong>, which scales the value range using multiplication, i.e.:<br>$$\begin{aligned}<br>    \hat{x}_ {nc} &amp;&#x3D; p_ {n}(x_ {nc}) &#x3D; p_ n x_ {nc}, \\<br>    \hat{w}_ {nd} &amp;&#x3D; r_ {d}(w_ {dc}) &#x3D; r_ d w_ {dc}, \\<br>    \hat{y}_ {dc} &amp;&#x3D; q_ {nd}(y_ {nd}) &#x3D; p_ n r_ d y_ {nd}.<br>\end{aligned}$$</p>
<p>We can formulate per-token symmetric uniform quantization by matrix multiplication:<br>$$\begin{aligned}<br>    \hat{\mathbf{X}} &amp;&#x3D; \text{diag}(p_1,\cdots,p_ N)\cdot \mathbf{X} &#x3D; \begin{pmatrix}<br>        p_ 1 &amp; \cdots &amp; p_ 1 \\<br>        \vdots &amp; \ddots &amp; \vdots \\<br>        p_ N &amp; \cdots &amp; p_ N<br>    \end{pmatrix} \otimes \mathbf{X}, \\<br>    \hat{\mathbf{W}} &amp;&#x3D; \text{diag}(r_1,\cdots,r_ D)\cdot \mathbf{W} &#x3D; \begin{pmatrix}<br>        r_ 1 &amp; \cdots &amp; r_ D \\<br>        \vdots &amp; \ddots &amp; \vdots \\<br>        r_ 1 &amp; \cdots &amp; r_ D<br>    \end{pmatrix} \otimes \mathbf{W}, \\<br>    \hat{\mathbf{Y}} &amp;&#x3D; \text{diag}(p_1,\cdots,p_ N)\cdot \mathbf{Y} \cdot \text{diag}(r_1,\cdots,r_ D) &#x3D; \begin{pmatrix}<br>        p_ 1 r_ 1 &amp; \cdots &amp; p_ 1 r_ D \\<br>        \vdots &amp; \ddots &amp; \vdots \\<br>        p_ N r_ 1 &amp; \cdots &amp; p_ N r_ D<br>    \end{pmatrix} \otimes \mathbf{Y},<br>\end{aligned}$$<br>where $\otimes$ represents element-wise matrix multiplication. It can be observed that both quantization and dequantization <strong>can be efficiently implemented using element-wise matrix multiplication with dimension broadcasting</strong>. The following figure illustrates the computation process by an example:</p>
<p><img src="/2024-03-06-quantization-gemm/quant_matrix.png"></p>
<h2 id="Hardware-requirements"><a href="#Hardware-requirements" class="headerlink" title="Hardware requirements"></a>Hardware requirements</h2><p>Hardware support still need to be considered when we try to utilize GEMM for quantization. For example, on NVIDIA GPUs, Tensor Core supports matrix multiplication for FP16 and INT8, but it doesn’t support mixed precision matrix multiplication for FP16&#x2F;INT8. This means that W8A8 quantization can benefit from Tensor Core, but W8A16 and W16A8 quantization lack hardware support and may not achieve real acceleration on NVIDIA GPUs. Many W8A16 and W16A8 quantization methods actually perform dequantization before GEMM and then use FP16 for computation. The actual acceleration effects of these methods require further discussion (see below).</p>
<h2 id="Performance-analysis"><a href="#Performance-analysis" class="headerlink" title="Performance analysis"></a>Performance analysis</h2><p>The above discussion only shows that per-token quantization can leverage GEMM. The following words will show whether it can provide actual acceleration.</p>
<p>We compare the following three setups:</p>
<ol>
<li>Unquantized, using FP16 for both storage and computation.</li>
<li>W8A8 quantization, with I&#x2F;O activations stored in FP16. This is the approach used by some works like <code>LLM.int8()</code>. To avoid additional CUDA kernel launch overhead, we assume that quantization and dequantization are fused with GEMM.</li>
<li>W8A16 quantization, internally converting weights to FP16 for computation. Kernel fusion is also applied here.</li>
</ol>
<p>Without loss of generality, we can assume that the hardware INT8 throughput is $2\times$ than that of FP16. We can set normalized operations of one INT8 operation is $1$, while $2$ for FP16. We can list the following table:</p>
<table>
<thead>
<tr>
<th align="center">Method</th>
<th align="center">FP16</th>
<th align="center">W8A8 (FP16 activations I&#x2F;O)</th>
<th align="center">W8A16</th>
</tr>
</thead>
<tbody><tr>
<td align="center">GEMM OPs</td>
<td align="center">$2NCD$</td>
<td align="center">$NCD$</td>
<td align="center">$2NCD$</td>
</tr>
<tr>
<td align="center">GEMM mem I&#x2F;O</td>
<td align="center">$2(NC+CD+ND)$</td>
<td align="center">$2NC+CD+2N D$</td>
<td align="center">$2NC+CD+2ND$</td>
</tr>
<tr>
<td align="center">quant&#x2F;dequant OPs</td>
<td align="center">$0$</td>
<td align="center">$2NC+4ND$</td>
<td align="center">$2CD$</td>
</tr>
<tr>
<td align="center">quant&#x2F;dequant Mem I&#x2F;O</td>
<td align="center">$0$</td>
<td align="center">$2(N+C_o)$</td>
<td align="center">$2D$</td>
</tr>
<tr>
<td align="center">total OPs</td>
<td align="center">$2NC D$</td>
<td align="center">$NC D+2NC+4N D$</td>
<td align="center">$2NCD+2CD$</td>
</tr>
<tr>
<td align="center">total mem I&#x2F;O</td>
<td align="center">$2(NC+C D+N D)$</td>
<td align="center">$2NC+C D+2N D+2(N+C_o)$</td>
<td align="center">$2NC+CD+2ND+2D$</td>
</tr>
<tr>
<td align="center">total arithmetic intensity (OPs:I&#x2F;O)</td>
<td align="center">$\cfrac{1}{1&#x2F;N+1&#x2F;C+1&#x2F;D}$</td>
<td align="center">$\cfrac{1+2&#x2F;D+4&#x2F;C}{2&#x2F;N+1&#x2F;C+2&#x2F;D+2&#x2F;(NC)+2&#x2F;(CD)}$</td>
<td align="center">$\cfrac{1+2&#x2F;N}{1&#x2F;(2N)+1&#x2F;C+1&#x2F;D+1&#x2F;(NC)}$</td>
</tr>
<tr>
<td align="center">total arithmetic intensity (second-order approximation)</td>
<td align="center">$\cfrac{1}{1&#x2F;N+1&#x2F;C+1&#x2F;D}$</td>
<td align="center">$\cfrac{1}{2&#x2F;N+1&#x2F;C+2&#x2F;D}$</td>
<td align="center">$\cfrac{1}{1&#x2F;(2N)+1&#x2F;C+1&#x2F;D}$</td>
</tr>
</tbody></table>
<p>Analyzing the table above, we can draw the following conclusions:</p>
<ol>
<li>W8A8 quantization (with FP16 activations I&#x2F;O) reduces the operations by almost half compared to FP16, but it decreases the total arithmetic intensity. Therefore, in memory-bound scenarios, W8A8 quantization may not achieve a $2\times$ throughput improvement (ZeroQuant addresses this issue, as discussed below). But <strong>it can still lead to a significant throughput improvement when memory bandwidth is sufficient</strong>.</li>
<li>W8A16 quantization maintains a similar operations compared to FP16, but it slightly increases the total arithmetic intensity (more increase when $N$ is large). Therefore, <strong>it also has practical value in memory-bound scenarios</strong>, especially since activations in LLMs are typically harder to be quantized than weights.</li>
</ol>
<h2 id="Some-LLM-Quantization-works"><a href="#Some-LLM-Quantization-works" class="headerlink" title="Some LLM Quantization works"></a>Some LLM Quantization works</h2><h3 id="LLM-int8"><a href="#LLM-int8" class="headerlink" title="LLM.int8()"></a><code>LLM.int8()</code></h3><p><code>LLM.int8()</code> actually employs selective per-token quantization. It stores weights and activations in FP16 and then applies different strategies for different tokens, as illustrated below:</p>
<p><img src="/2024-03-06-quantization-gemm/llm_int8.png" alt="LLM.int8()"></p>
<ul>
<li>For tokens suitable for quantization, it applies per-token INT8 quantization to weights and activations, computes results using INT8 GEMM, and then dequantizes them to FP16.</li>
<li>For tokens with outliers, it directly computed the FP16 GEMM.</li>
</ul>
<p>The results from these two parts can be combined to form the final result.</p>
<h3 id="SmoothQuant"><a href="#SmoothQuant" class="headerlink" title="SmoothQuant"></a>SmoothQuant</h3><p>While per-channel quantization may not be practical, for LLM activation quantization, the main challenge arises from activations, where values with larger magnitudes may appear on some channels, as shown below:</p>
<p><img src="/2024-03-06-quantization-gemm/smooth_quant_motivation.png"></p>
<p>SmoothQuant observed that these outliers occur consistently in specific channels, while outliers are rare in weights (thus easier to quantize). Therefore, it proposes to “balance” the quantization difficulty between activations and weights by introducing a per-channel scaling factor:</p>
<p><img src="/2024-03-06-quantization-gemm/smooth_quant.png" alt="SmoothQuant"></p>
<p>This “balance” can be formulated as:<br>$$\begin{aligned}<br>    \mathbf{Y}<br>    &amp;&#x3D; \mathbf{X}\mathbf{W}^\top \\<br>    &amp;&#x3D; \mathbf{X} \cdot \text{diag}(s_ 1,\cdots,s_ C) \cdot \text{diag}(s_ 1,\cdots,s_ C)^{-1} \cdot \mathbf{W}^\top \\<br>    &amp; &#x3D; \left( \mathbf{X} \cdot \text{diag}(s_ 1,\cdots,s_ C) \right) \cdot \left( \mathbf{W}\cdot \text{diag}(s_ 1,\cdots,s_ C)^{-1} \right)^\top.<br>\end{aligned}$$<br>By selecting appropriate scaling factors $\text{diag}(s_ 1,\cdots,s_ C)$, we can achieve the goal of balancing outlier values in activations, and then we can quantize $\mathbf{X} \cdot \text{diag}(s_ 1,\cdots,s_ C)$ and $\mathbf{W}\cdot \text{diag}(s_ 1,\cdots,s_ C)^{-1}$. The following figure give an example:</p>
<p><img src="/2024-03-06-quantization-gemm/smooth_quant_2.png" alt="SmoothQuant example"></p>
<p><strong>SmoothQuant is an excellent alternative to per-channel quantization</strong>, as demonstrated in the paper by its impressive performance in quantizing LLM to W8A8.</p>
<h3 id="ZeroQuant"><a href="#ZeroQuant" class="headerlink" title="ZeroQuant"></a>ZeroQuant</h3><p>In the above performance analysis of W8A8, we found that using FP16 for activations I&#x2F;O reduces the overall arithmetic intensity after quantization, which may harm the throughput improvement in memory-bound scenarios. ZeroQuant addresses this issue by fusing the quantization into the previous operator and fusing the dequantization after GEMM, as shown in the figure below.</p>
<p><img src="/2024-03-06-quantization-gemm/zero_quant.png" alt="ZeroQuant"></p>
<p>Thus, the activations I&#x2F;O between operators are still INT8, which reduces the total memory I&#x2F;O to $NC+CD+ND+2(N+D)$, boosting arithmetic intensity to original FP16 level , and fully leveraging the high throughput of INT8.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>This blog provides a matrix multiplication perspective for quantization, indicating some fundamental requirements for practical quantization and explaining why per-channel quantization in impractical. It also discusses several examples of LLM per-token quantization, including <code>LLM.int8()</code>, SmoothQuant, and ZeroQuant.<br>They are all practical and demonstrate significant acceleration in real-world scenarios.</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>Post author:  </strong>Monsoon
  </li>
  <li class="post-copyright-link">
      <strong>Post link: </strong>
      <a href="https://monsoon-cs.moe/2024-03-06-quantization-gemm/" title="How Quantization Works: From a Matrix Multiplication Perspective">https://monsoon-cs.moe/2024-03-06-quantization-gemm/</a>
  </li>
  <li class="post-copyright-license">
      <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/ml-system/" rel="tag"># ml-system</a>
              <a href="/tags/llm/" rel="tag"># llm</a>
              <a href="/tags/quantization/" rel="tag"># quantization</a>
              <a href="/tags/gemm/" rel="tag"># gemm</a>
              <a href="/tags/cuda-kernel/" rel="tag"># cuda-kernel</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024-02-16-nfs-tuning/" rel="prev" title="NFS Performance Tuning">
                  <i class="fa fa-angle-left"></i> NFS Performance Tuning
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024-07-07-latency-in-llm-serving/" rel="next" title="Latency in LLM Serving">
                  Latency in LLM Serving <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Monsoon</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/monsoon235" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
