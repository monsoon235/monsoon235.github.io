<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.33/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"monsoon-cs.moe","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Introduction量化是一种神经网络推理常用的加速技术。 神经网络中主要的计算量来自卷积、线性层、Attention 等，这些算子在底层都是由 GEMM&#x2F;BMM 等实现的。本文旨在从矩阵乘法角度讨论量化（以及反量化）的原理，并说明为什么有些量化是没有实用价值的，也旨在从这个角度重新审视一些 LLM 中的量化方法。 我把实用的量化定义为：  量化后仍然可以使用 GEMM&#x2F;B">
<meta property="og:type" content="article">
<meta property="og:title" content="How Quantization Works: From a Matrix Multiplication Perspective">
<meta property="og:url" content="https://monsoon-cs.moe/2024-03-06-quantization-gemm/">
<meta property="og:site_name" content="Monsoon&#39;s Blog">
<meta property="og:description" content="Introduction量化是一种神经网络推理常用的加速技术。 神经网络中主要的计算量来自卷积、线性层、Attention 等，这些算子在底层都是由 GEMM&#x2F;BMM 等实现的。本文旨在从矩阵乘法角度讨论量化（以及反量化）的原理，并说明为什么有些量化是没有实用价值的，也旨在从这个角度重新审视一些 LLM 中的量化方法。 我把实用的量化定义为：  量化后仍然可以使用 GEMM&#x2F;B">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://monsoon-cs.moe/2024-03-06-quantization-gemm/quant_matrix.png">
<meta property="og:image" content="https://monsoon-cs.moe/2024-03-06-quantization-gemm/llm_int8.png">
<meta property="og:image" content="https://monsoon-cs.moe/2024-03-06-quantization-gemm/smooth_quant_motivation.png">
<meta property="og:image" content="https://monsoon-cs.moe/2024-03-06-quantization-gemm/smooth_quant.png">
<meta property="og:image" content="https://monsoon-cs.moe/2024-03-06-quantization-gemm/smooth_quant_2.png">
<meta property="og:image" content="https://monsoon-cs.moe/2024-03-06-quantization-gemm/zero_quant.png">
<meta property="article:published_time" content="2024-03-05T16:00:00.000Z">
<meta property="article:modified_time" content="2024-03-05T16:00:00.000Z">
<meta property="article:author" content="Monsoon">
<meta property="article:tag" content="ml-system">
<meta property="article:tag" content="llm">
<meta property="article:tag" content="quantization">
<meta property="article:tag" content="gemm">
<meta property="article:tag" content="cuda-kernel">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://monsoon-cs.moe/2024-03-06-quantization-gemm/quant_matrix.png">


<link rel="canonical" href="https://monsoon-cs.moe/2024-03-06-quantization-gemm/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://monsoon-cs.moe/2024-03-06-quantization-gemm/","path":"2024-03-06-quantization-gemm/","title":"How Quantization Works: From a Matrix Multiplication Perspective"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>How Quantization Works: From a Matrix Multiplication Perspective | Monsoon's Blog</title>
  



  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{&quot;token&quot;: &quot;3c9e3103682b4f6fbc36342486e67640&quot;}'></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Monsoon's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Monsoon's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Let%E2%80%99s-do-some-math"><span class="nav-number">2.</span> <span class="nav-text">Let’s do some math</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Some-basic-quantization-methods"><span class="nav-number">3.</span> <span class="nav-text">Some basic quantization methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Per-element-and-Per-channel"><span class="nav-number">3.1.</span> <span class="nav-text">Per-element and Per-channel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Per-token-and-per-tensor"><span class="nav-number">3.2.</span> <span class="nav-text">Per-token and per-tensor</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hardware-requirements"><span class="nav-number">4.</span> <span class="nav-text">Hardware requirements</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Performance-analysis"><span class="nav-number">5.</span> <span class="nav-text">Performance analysis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B-LLM-%E9%87%8F%E5%8C%96%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-number">6.</span> <span class="nav-text">一些 LLM 量化的例子</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LLM-int8"><span class="nav-number">6.1.</span> <span class="nav-text">LLM.int8()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SmoothQuant"><span class="nav-number">6.2.</span> <span class="nav-text">SmoothQuant</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ZeroQuant"><span class="nav-number">6.3.</span> <span class="nav-text">ZeroQuant</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">7.</span> <span class="nav-text">总结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Monsoon</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">40</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/monsoon235" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;monsoon235" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://t.me/monsoon235" title="Telegram → https:&#x2F;&#x2F;t.me&#x2F;monsoon235" rel="noopener me" target="_blank"><i class="fab fa-telegram fa-fw"></i>Telegram</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/big/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://monsoon-cs.moe/2024-03-06-quantization-gemm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Monsoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="How Quantization Works: From a Matrix Multiplication Perspective | Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          How Quantization Works: From a Matrix Multiplication Perspective
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-03-06 00:00:00" itemprop="dateCreated datePublished" datetime="2024-03-06T00:00:00+08:00">2024-03-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>量化是一种神经网络推理常用的加速技术。</p>
<p>神经网络中主要的计算量来自卷积、线性层、Attention 等，这些算子在底层都是由 GEMM&#x2F;BMM 等实现的。本文旨在<strong>从矩阵乘法角度讨论量化（以及反量化）的原理，并说明为什么有些量化是没有实用价值的</strong>，也旨在从这个角度重新审视一些 LLM 中的量化方法。</p>
<p>我把<strong>实用的量化</strong>定义为：</p>
<ol>
<li>量化后仍然<strong>可以使用 GEMM&#x2F;BMM 完成运算</strong>。这是能取得加速效果的基本要求。</li>
<li>可以产生<strong>实际加速效果</strong>。加速可以来自更高的 INT8 硬件算力，也可以来自 INT8 更小的 memory footprint 所节约的内存带宽。重要的是，加速收益必须能大于量化开销。</li>
</ol>
<h2 id="Let’s-do-some-math"><a href="#Let’s-do-some-math" class="headerlink" title="Let’s do some math"></a>Let’s do some math</h2><p>设神经网络中的某一个算子可以写成矩阵乘法：<br>$$\mathbf{Y}&#x3D;\mathbf{X} \mathbf{W}^\top,$$<br>其中 $\mathbf{X} \in \mathbb{R}^{N \times C}$，$\mathbf{Y} \in \mathbb{R}^{N \times D}$，$\mathbf{W} \in \mathbb{R}^{D \times C}$，同时记量化后的版本为 $\hat{\mathbf{X}}$，$\hat{\mathbf{Y}}$，$\hat{\mathbf{W}}$。</p>
<p>我们的目标是量化后仍然能用 GEMM&#x2F;BMM 完成运算，即：<br>$$\hat{\mathbf{Y}}&#x3D;\hat{\mathbf{X}} \hat{\mathbf{W}}^\top.$$<br>设 $\mathbf{X}$，$\mathbf{Y}$，$\mathbf{W}$ 的 per-element 量化函数分别为 $p_ {nc}(\cdot)$，$q_ {nd}(\cdot)$，$r_ {dc}(\cdot)$，即<br>$$\begin{aligned}<br>    \hat{x}_ {nc} &amp;&#x3D; p_ {nc}(x_{nc}), \\<br>    \hat{y}_ {nd} &amp;&#x3D; q_ {nd}(y_{nd}), \\<br>    \hat{w}_ {dc} &amp;&#x3D; r_ {dc}(w_{dc}).<br>\end{aligned}$$<br>对应的反量化函数记为 $p_ {nc}^{-1}(\cdot)$，$q_ {nd}^{-1}(\cdot)$，$r_ {dc}^{-1}(\cdot)$。那么有<br>$$\begin{aligned}<br>y_ {nd}<br>&amp;&#x3D; \sum_ {c&#x3D;1}^{C_ i} x_ {nc} w_ {dc}, \\<br>q_ {nd}^{-1}(\hat{y}_ {nd}) &amp;&#x3D; \sum_ {c&#x3D;1}^{C_ i} p_ {nc}^{-1}(\hat{x}_ {nc}) r_ {dc}^{-1}(\hat{w}_ {dc}).<br>\end{aligned}$$<br>上述公式即是<strong>实用的量化</strong>所需要满足的<strong>基本约束条件</strong>。</p>
<h2 id="Some-basic-quantization-methods"><a href="#Some-basic-quantization-methods" class="headerlink" title="Some basic quantization methods"></a>Some basic quantization methods</h2><p>有了这个基本约束条件，我们就能讨论几种基本的量化方法。</p>
<h3 id="Per-element-and-Per-channel"><a href="#Per-element-and-Per-channel" class="headerlink" title="Per-element and Per-channel"></a>Per-element and Per-channel</h3><p>上述公式中，左边的反量化函数 $q_ {nd}^{-1}(\cdot)$ 与 $c$ 无关。<br>显然如果右侧的量化函数 $p_ {nc}^{-1}(\cdot)$ 和 $r_ {dc}^{-1}(\cdot)$ 与 $c$ 有关，这个约束条件将不成立。这意味着满足我们「可利用 GEMM&#x2F;BMM 完成运算」约束条件的、并且在不同 $\mathbf{X}$ 和 $\mathbf{W}$ 的 channel 上不同的量化函数是不存在的。</p>
<p>换句话说，这说明了 <strong>per-element 和 per-channel 量化不能用 GEMM&#x2F;BMM 加速，他们不具备实用价值</strong>。</p>
<h3 id="Per-token-and-per-tensor"><a href="#Per-token-and-per-tensor" class="headerlink" title="Per-token and per-tensor"></a>Per-token and per-tensor</h3><p>由上述讨论可知，实用的量化至少需要保证：<br>$$\begin{aligned}<br>    p_ {n}(\cdot) &amp;&#x3D; p_ {nc} (\cdot), \quad \forall n, c, \\<br>    r_ {d}(\cdot) &amp;&#x3D; r_ {dc} (\cdot), \quad \forall d, c,<br>\end{aligned}$$<br>即<strong>每个 channel 上的量化函数相同</strong>，那么基本约束条件化为：<br>$$q_ {nd}^{-1}(\hat{y}_ {nd}) &#x3D; \sum_ {c&#x3D;1}^{C_ i} p_ {n}^{-1}(\hat{x}_ {nc}) r_ {d}^{-1}(\hat{w}_ {dc}),$$<br>如此，我们得到了 <strong>per-token 量化</strong>。如果我们进一步设<br>$$\begin{aligned}<br>    p(\cdot) &amp;&#x3D; p_ {nc} (\cdot), \quad \forall n, c, \\<br>    r(\cdot) &amp;&#x3D; r_ {dc} (\cdot), \quad \forall d, c,<br>\end{aligned}$$<br>即<strong>量化函数对整个 $\mathbf{X}$ 和 $\mathbf{W}$ 中的元素都相同</strong>，那么基本约束条件化为：<br>$$q_ {nd}^{-1}(\hat{y}_ {nd}) &#x3D; q^{-1}(\hat{y}_ {nd}) &#x3D; \sum_ {c&#x3D;1}^{C_i} p^{-1}(\hat{x}_ {nc}) r^{-1}(\hat{w}_ {dc}),$$<br>我们就得到了 <strong>per-tensor 量化</strong>。这两种量化存在理论上的可行性，但实际使用还需要考虑硬件的支持（见下一节）。</p>
<p>方便起见，下文只讨论 per-token 量化，per-tensor 量化可以视为 per-token 量化的特例。实际中最常用的量化方式是<strong>对称均匀量化</strong>，它使用乘法实现 scale 缩放：<br>$$\begin{aligned}<br>    \hat{x}_ {nc} &amp;&#x3D; p_ {n}(x_ {nc}) &#x3D; p_ n x_ {nc}, \\<br>    \hat{w}_ {nd} &amp;&#x3D; r_ {d}(w_ {dc}) &#x3D; r_ d w_ {dc}, \\<br>    \hat{y}_ {dc} &amp;&#x3D; q_ {nd}(y_ {nd}) &#x3D; p_ n r_ d y_ {nd}.<br>\end{aligned}$$</p>
<p>我们可以把 per-token 量化写成矩阵乘法的形式：<br>$$\begin{aligned}<br>    \hat{\mathbf{X}} &amp;&#x3D; \text{diag}(p_1,\cdots,p_ N)\cdot \mathbf{X} &#x3D; \begin{pmatrix}<br>        p_ 1 &amp; \cdots &amp; p_ 1 \\<br>        \vdots &amp; \ddots &amp; \vdots \\<br>        p_ N &amp; \cdots &amp; p_ N<br>    \end{pmatrix} \otimes \mathbf{X}, \\<br>    \hat{\mathbf{W}} &amp;&#x3D; \text{diag}(r_1,\cdots,r_ D)\cdot \mathbf{W} &#x3D; \begin{pmatrix}<br>        r_ 1 &amp; \cdots &amp; r_ D \\<br>        \vdots &amp; \ddots &amp; \vdots \\<br>        r_ 1 &amp; \cdots &amp; r_ D<br>    \end{pmatrix} \otimes \mathbf{W}, \\<br>    \hat{\mathbf{Y}} &amp;&#x3D; \text{diag}(p_1,\cdots,p_ N)\cdot \mathbf{Y} \cdot \text{diag}(r_1,\cdots,r_ D) &#x3D; \begin{pmatrix}<br>        p_ 1 r_ 1 &amp; \cdots &amp; p_ 1 r_ D \\<br>        \vdots &amp; \ddots &amp; \vdots \\<br>        p_ N r_ 1 &amp; \cdots &amp; p_ N r_ D<br>    \end{pmatrix} \otimes \mathbf{Y},<br>\end{aligned}$$<br>其中 $\otimes$ 代表 element-wise matrix multiplication。可以看到这些量化和反量化都<strong>可以用维度广播的 element-wise matrix multiplication 高效实现</strong>，下图使用了一个例子展示了其计算过程：</p>
<p><img src="/2024-03-06-quantization-gemm/quant_matrix.png"></p>
<h2 id="Hardware-requirements"><a href="#Hardware-requirements" class="headerlink" title="Hardware requirements"></a>Hardware requirements</h2><p>量化是否可以使用 GEMM&#x2F;BMM 还需要考虑硬件的支持。例如，在 NVIDIA GPU 上，Tensor Core 支持 FP16 和 INT8 的矩阵乘，但不支持 FP16&#x2F;INT8 混合精度矩阵乘。这意味着 W8A8 量化可以使用 Tensor Core 加速，但 W8A16、W16A8 缺少硬件加速的支持，在 NVIDIA GPU 上并不能取得加速效果。</p>
<h2 id="Performance-analysis"><a href="#Performance-analysis" class="headerlink" title="Performance analysis"></a>Performance analysis</h2><p>以上讨论只说明了 per-token 量化可以使用 GEMM&#x2F;BMM 计算，下文将讨论它是否具备实际加速效果。</p>
<p>为了体现量化和反量化本身的开销的影响，我们假设使用量化的情况下每层的输入和输出 activation 仍然都是 FP16 的（这是某些量化方法——如 <code>LLM.int8()</code>——所使用的）。不失一般性，我们可以假设硬件的 INT8 吞吐量是 FP16 的两倍，即一次 INT8 FMA 运算的归一化操作数是 $1$，而 FP16 是 $2$。再假设我们将量化&#x2F;反量化操作和GEMM&#x2F;BMM 做了算子融合（因而可以两种情况下 CUDA kernel launch 开销的不同）。针对上面的 per-token 量化，我们可以列出下表：</p>
<table>
<thead>
<tr>
<th align="center">计算方式</th>
<th align="center">FP16</th>
<th align="center">INT8 量化（FP16 输入输出）</th>
</tr>
</thead>
<tbody><tr>
<td align="center">GEMM&#x2F;BMM OPs</td>
<td align="center">$2NC_ i C_ o$</td>
<td align="center">$NC_ i C_ o$</td>
</tr>
<tr>
<td align="center">GEMM&#x2F;BMM I&#x2F;O</td>
<td align="center">$2(NC_ i+C_ i C_ o+N C_ o)$</td>
<td align="center">$2NC_ i+C_ i C_ o+2N C_ o$</td>
</tr>
<tr>
<td align="center">量化和反量化 OPS</td>
<td align="center">$0$</td>
<td align="center">$2NC_ i+4N C_ o$</td>
</tr>
<tr>
<td align="center">量化和反量化 I&#x2F;O</td>
<td align="center">$0$</td>
<td align="center">$2(N+C_o)$</td>
</tr>
<tr>
<td align="center">总 OPS</td>
<td align="center">$2NC_ i C_ o$</td>
<td align="center">$NC_ i C_ o+2NC_ i+4N C_ o$</td>
</tr>
<tr>
<td align="center">总 I&#x2F;O</td>
<td align="center">$2(NC_ i+C_ i C_ o+N C_ o)$</td>
<td align="center">$2NC_ i+C_ i C_ o+2N C_ o+2(N+C_o)$</td>
</tr>
<tr>
<td align="center">总算术强度（OPS:I&#x2F;O）</td>
<td align="center">$\cfrac{1}{1&#x2F;N+1&#x2F;C_ i+1&#x2F;C_ o}$</td>
<td align="center">$\cfrac{1+2&#x2F;C_ o+4&#x2F;C_ i}{2&#x2F;N+1&#x2F;C_ i+2&#x2F;C_ o+2&#x2F;(NC_ i)+2&#x2F;(C_ iC_ o)}$</td>
</tr>
</tbody></table>
<p>可以看出，量化后的<strong>总算术强度变小了</strong>，因而计算更有可能是 memory-bound 的（SmoothQuant 改进了这一点，后文会介绍）。<strong>在内存带宽充足的情况下，对称均匀量化大致降低了一半的总计算量，将会带来可观的加速效果。</strong></p>
<h2 id="一些-LLM-量化的例子"><a href="#一些-LLM-量化的例子" class="headerlink" title="一些 LLM 量化的例子"></a>一些 LLM 量化的例子</h2><h3 id="LLM-int8"><a href="#LLM-int8" class="headerlink" title="LLM.int8()"></a><code>LLM.int8()</code></h3><p><code>LLM.int8()</code> 实质上使用了选择性 per-token 量化。它使用 FP16 存储 weights 和 activations，然后对于不同的 token 采用不同的策略（如下图所示）：</p>
<p><img src="/2024-03-06-quantization-gemm/llm_int8.png" alt="LLM.int8()"></p>
<ul>
<li>对于适合量化的 token，它对 weights 和 activations 使用了 per-token 的 INT8 量化，并使用 INT8 GEMM 计算得到结果后反量化成 FP16；</li>
<li>对于包含离群值的 token，则直接使用 FP16 进行计算。</li>
</ul>
<p>这两部分的结果即可合成最终结果。</p>
<h3 id="SmoothQuant"><a href="#SmoothQuant" class="headerlink" title="SmoothQuant"></a>SmoothQuant</h3><p>虽然 per-channel 量化并不实用，但对于 LLM activation 量化来说，最大的挑战来自于 activation 在一些 channel 上会出现数量级较大的值（如下图）。</p>
<p><img src="/2024-03-06-quantization-gemm/smooth_quant_motivation.png"></p>
<p>SmoothQuant 发现这些离群值出现的 channel 非常固定，但同时 weights 中很少会有离群值（因而容易量化），因此它提出在 activations 和 weights 之间平衡量化的难度，通过平衡 per-channel 的数量级。</p>
<p><img src="/2024-03-06-quantization-gemm/smooth_quant.png" alt="SmoothQuant"></p>
<p>这种「平衡」可以用数学表述：<br>$$\begin{aligned}<br>    \mathbf{Y}<br>    &amp;&#x3D; \mathbf{X}\mathbf{W}^\top \\<br>    &amp;&#x3D; \mathbf{X} \cdot \text{diag}(s_ 1,\cdots,s_ C) \cdot \text{diag}(s_ 1,\cdots,s_ C)^{-1} \cdot \mathbf{W}^\top \\<br>    &amp; &#x3D; \left( \mathbf{X} \cdot \text{diag}(s_ 1,\cdots,s_ C) \right) \cdot \left( \mathbf{W}\cdot \text{diag}(s_ 1,\cdots,s_ C)^{-1} \right)^\top.<br>\end{aligned}$$<br>通过选取合适的 $\text{diag}(s_ 1,\cdots,s_ C)$，我们就能达成平衡离群值数量级的目的，随后我们就能对 $\mathbf{X} \cdot \text{diag}(s_ 1,\cdots,s_ C)$ 和 $\mathbf{W}\cdot \text{diag}(s_ 1,\cdots,s_ C)^{-1}$ 进行量化。下图是这个过程的一个例子：</p>
<p><img src="/2024-03-06-quantization-gemm/smooth_quant_2.png" alt="SmoothQuant example"></p>
<p>SmoothQuant 是 per-channel 量化的一种绝佳替代，论文也展示了它在 LLM W8A8 量化中令人印象深刻的表现。</p>
<h3 id="ZeroQuant"><a href="#ZeroQuant" class="headerlink" title="ZeroQuant"></a>ZeroQuant</h3><p>在上述对 practical quantization 的 performance analysis，我们发现如果使用 FP16 传递 activation，则量化后的总算数强度变小了，可能会导致 memory bound 从而影响性能。ZeroQuant 则把量化过程融合到前一个算子后，而反量化过程融合到 GEMM 之后，如下图所示。如此，算子之间的传递的 activation 仍然是 INT8 的，这样可以将总的访存量降到 $NC_ i+C_ i C_ o+N C_ o+2(N+C_o)$，算术强度提升到和未量化的接近的水平，充分发挥 INT8 高吞吐量的优势。</p>
<p><img src="/2024-03-06-quantization-gemm/zero_quant.png" alt="ZeroQuant"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文从矩阵乘法的角度梳理了量化，并说明了实用的量化需要满足的一些基本条件，也说明了 per-channel 量化不实用的原因。本文还讨论了 per-token 量化的一些例子，包括 <code>LLM.int8()</code>，SmoothQuant 和 ZeroQuant。这些例子都是实用的量化方法，并且在实际中取得了可观的加速效果。</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Monsoon
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://monsoon-cs.moe/2024-03-06-quantization-gemm/" title="How Quantization Works: From a Matrix Multiplication Perspective">https://monsoon-cs.moe/2024-03-06-quantization-gemm/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/ml-system/" rel="tag"># ml-system</a>
              <a href="/tags/llm/" rel="tag"># llm</a>
              <a href="/tags/quantization/" rel="tag"># quantization</a>
              <a href="/tags/gemm/" rel="tag"># gemm</a>
              <a href="/tags/cuda-kernel/" rel="tag"># cuda-kernel</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024-02-16-nfs-tuning/" rel="prev" title="NFS Performance Tuning">
                  <i class="fa fa-angle-left"></i> NFS Performance Tuning
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Monsoon</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/monsoon235" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.33/fancybox/fancybox.umd.js" integrity="sha256-+2+qOqR8CKoHh/AsVR9k2qaDBKWjYNC2nozhYmv5j9k=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
