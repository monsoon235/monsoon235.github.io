<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.28/fancybox/fancybox.css" integrity="sha256-6cQIC71/iBIYXFK+0RHAvwmjwWzkWd+r7v/BX3/vZDc=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"monsoon-cs.moe","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="Monsoon&#39;s Blog">
<meta property="og:url" content="https://monsoon-cs.moe/">
<meta property="og:site_name" content="Monsoon&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Monsoon">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://monsoon-cs.moe/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Monsoon's Blog</title>
  



  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{&quot;token&quot;: &quot;3c9e3103682b4f6fbc36342486e67640&quot;}'></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Monsoon's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Monsoon</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">35</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/monsoon235" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;monsoon235" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://t.me/monsoon235" title="Telegram → https:&#x2F;&#x2F;t.me&#x2F;monsoon235" rel="noopener me" target="_blank"><i class="fab fa-telegram fa-fw"></i>Telegram</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/big/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://monsoon-cs.moe/2024-02-07-paper-reading-pact22-gpupool/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Monsoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024-02-07-paper-reading-pact22-gpupool/" class="post-title-link" itemprop="url">[Paper Reading] GPUPool: A Holistic Approach to Fine-Grained GPU Sharing in the Cloud (PACT'22)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2024-02-07 00:00:00 / Modified: 04:33:26" itemprop="dateCreated datePublished" datetime="2024-02-07T00:00:00+08:00">2024-02-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>This blog is a write-up of the paper “<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3559009.3569650">GPUPool: A Holistic Approach to Fine-Grained GPU Sharing in the Cloud</a>“ from PACT’22.</p>
</blockquote>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>This paper focuses on the <strong>GPU sharing in cloud scenarios</strong>.</p>
<p>Currently, existing GPU sharing techniques can be categorized into 2 types:</p>
<ul>
<li><p><strong>Time-sharing</strong> means executing each concurrent VM on a full device in a round-robin fashion. <strong>Pros</strong>: Simple and mature. <strong>Cons</strong>: VMs could still under-utilize the hardware within each time slice.</p>
</li>
<li><p><strong>Shape-sharing</strong>: split a device into partitions and allows multiple workloads to execute on different partitions simultaneously.</p>
</li>
</ul>
<p>Space-sharing can be categorized into 2 types：</p>
<ul>
<li><p><strong>Coarse-grained</strong> assigns disjoint sets of streaming multiprocessors (SMs) and memory channels to concurrent workloads. For example, Nvidia MIG. <strong>Pros</strong>: offers great performance isolation among tenants of the same GPU. <strong>Cons</strong>: (i) resource under-utilization within each SM consisting of heterogeneous functional units (e.g., FP32, INT, FP64, Tensor Cores) meant for different workload types. (ii) inefficient memory bandwidth usage caused by the bursty nature of GPU memory traffic.</p>
</li>
<li><p><strong>Fine-grained</strong> allows different workloads to co-run on the same SMs and request memory bandwidth flexibly, such as CUDA Stream and MPS. <strong>Pros</strong>: Better hardware utilization.</p>
</li>
</ul>
<p>The key problem of GPU sharing in data center is <strong>performance unpredictability</strong>. It contains 2 <strong>key challenges</strong>:</p>
<ol>
<li><p><strong>Mitigating interference</strong>. The amount of performance improvement from fine-grained sharing varies drastically depending on how compatible the concurrent workloads are in terms of resource usage. Also, the interference cannot be statically estimated. So, <strong>it is non-trivial to determine compatibility</strong> among a large number of incoming jobs in the cluster.</p>
</li>
<li><p><strong>Providing QoS guarantees</strong>.</p>
</li>
</ol>
<p>Existing solutions:</p>
<ul>
<li><p><strong>Software-based</strong>: kernel slicing or a persistent thread model. <strong>Cons</strong>: high scheduling overhead.</p>
</li>
<li><p><strong>Hardware-based</strong>: integrate sophisticated resource management logic into hardware to allocate resources for concurrent kernels. <strong>Cons</strong>: expensive and also inflexible.</p>
</li>
</ul>
<p>Common problems of existing solutions:</p>
<ol>
<li><p>They do not concern with interference mitigation at the cluster level.</p>
</li>
<li><p>They do not handle scenarios where incoming jobs must be distributed among multiple GPUs to satisfy QoS constraints.</p>
</li>
</ol>
<p><img src="/2024-02-07-paper-reading-pact22-gpupool/tb_sm.png" alt="Figure 1. Simulated system throughput of co-running `parb_spmv` and `rod_hotspot` at various TBs/SM settings"></p>
<p><strong>Problems of hardware TB scheduler</strong> which hinder the fine-grained sharing:</p>
<ol>
<li><p>It always attempts to <strong>launch as many thread blocks per SM</strong> (TBs&#x2F;SM) for each kernel as allowed by the execution context storage constraints (e.g., registers, shared memory, thread slots). <strong>It leaves insufficient resources for concurrent kernels</strong>. As showed in Figure 1, if we can individually set the TBs&#x2F;SM for each kernel, we may achieve a higher throughput.</p>
</li>
<li><p>It only dispatches concurrent kernels onto SMs after the earlier arriving one completes launching all the thread blocks specified by the kernel grid size. This will force an <strong>almost serially execution</strong> of kernels in some scenarios.</p>
</li>
</ol>
<p>GPU applications in the cloud fall into two main categories: latency-sensitive, and <strong>throughput-oriented</strong>. Throughput-oriented workloads are good candidates for hardware space-sharing. They have the following characteristics:</p>
<ol>
<li><p>Most workloads involve a large variety of kernels with <strong>different hardware resource utilization</strong> characteristics (e.g., CNN: compute-intensive, batch-norm: memory-intensive).</p>
</li>
<li><p>Active SMs are <strong>underutilized</strong> in some resources (FP, tensor core, memory bandwidth).</p>
</li>
<li><p>They typically repeatedly execute the same sequence of kernels (e.g., ML).</p>
</li>
<li><p>Relaxed QoS Requirements.</p>
</li>
</ol>
<h2 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h2><p>This paper proposed a <strong>hardware-software co-designed</strong> strategy to solve these challenges.</p>
<h3 id="Hardware"><a href="#Hardware" class="headerlink" title="Hardware"></a>Hardware</h3><p>This paper changes the default behavior of CUDA runtime to make it more suitable for fine-grained sharing:</p>
<ol>
<li><p>Allows CUDA runtime to program the <strong>TBs&#x2F;SM setting</strong> as one of the kernel launch parameters. The value of TBs&#x2F;SM is selected by the performance predictor.</p>
</li>
<li><p>Make the TB scheduler <strong>launch TBs from any concurrent kernels</strong> whenever they are running under their TBs&#x2F;SM quota.</p>
</li>
</ol>
<h3 id="Software"><a href="#Software" class="headerlink" title="Software"></a>Software</h3><blockquote>
<p>Concept Explanation:</p>
<ul>
<li>Job: a task submitted by user, such as a DNN training task. It may be iterative and contains multiple kernels.</li>
<li>Kernel: CUDA kernel.</li>
<li>Normalized Progress (NP): $t_{isolate} &#x2F; t_{co-execute}$.</li>
</ul>
</blockquote>
<p><strong>Two key observations</strong>:</p>
<ol>
<li><p>Co-execution performance of GPU kernels is highly correlated with resource utilization of individual kernels measured when running in isolation.</p>
</li>
<li><p>Once we have predicted which job pairs can co-execute without violating QoS requirements, the scheduling task can be reduced to the classic maximum cardinality matching problem in graph theory.</p>
</li>
</ol>
<p><img src="/2024-02-07-paper-reading-pact22-gpupool/system-design.png" alt="Figure 2. Overall System Design of GPUPool"></p>
<p>Based on these 2 observations, the author proposed GPUPool. Its overall system design is shown in Figure 2. It consists of 4 steps:</p>
<ol>
<li><p><strong>Kernel Profiler</strong>. GPUPool <strong>groups all incoming GPU job into a batch</strong> for every scheduling window (e.g., 30 seconds). User should provide application executable and execution time budget. Then GPUPool automatically <strong>profiles</strong> the application for one iteration of the job in isolation on hardware, to collect the <strong>performance counter metrics</strong> of each kernel of data.</p>
</li>
<li><p><strong>Co-execution Performance Predictor</strong>. This step decides the <strong>compatibility</strong> of all possible job pairs within the batch using the profiling result. It contains 2 stages:</p>
<ol>
<li><p><strong>Kernel-wise Predictors</strong>. It predicts how well each kernel from one job will co-run with the ones in the other job. This stage uses a <em>Gradient Boosting Tree</em> (GBT) model to <strong>predict the performance of each kernel when co-running with another kernel</strong> (based on the 1st key observation). The model takes the profiling data of kernels as input and outputs the <strong>NP</strong>. This prediction will be done for <strong>each feasible TBs&#x2F;SM</strong> settings.</p>
</li>
<li><p><strong>Job-wise Predictor</strong>. It gets an <em>interference matrix</em> (shown in Figure 3) based on the <strong>predicted NP</strong> (under optimal TBs&#x2F;SM setting) from former stage, which indicates how will two kernels slow down when they are co-running. Then, GPUPool using this matrix to calculate the <strong>co-running time of two jobs</strong>. Here, the authors found that a whole calculation may require tens of thousands iterations, but the result will <strong>coverage to a steady-state</strong> after several iterations. So the authors used an <strong>approximation algorithm</strong> (shown in Figure 4) – stops timeline calculation once the accumulated slowdown values of each job is within a small delta over the past epoch.</p>
</li>
</ol>
</li>
</ol>
<p><img src="/2024-02-07-paper-reading-pact22-gpupool/interference_matrix.png" alt="Figure 3. Interference Matrix"></p>
<p><img src="/2024-02-07-paper-reading-pact22-gpupool/stage2.2.png" alt="Figure 4. Concurrent Application Timeline"></p>
<ol start="3">
<li><strong>Job dispatcher</strong>. It decides which job pairs should co-run to maximize system performance while satisfying QoS. The decisions are found by solving a <strong>maximum cardinality matching problem</strong> – each node represent a job, when two jobs can co-run and will not violate the QoS requirement, connecting an edge between them. Then a graph theory algorithm is used to maximum cardinality matching, which means a largest subset of edges that do not share a common end node. Due to the potential unreliability of the performance predictor, GPUPool also add <strong>a safety margin</strong> $\delta$ to edge formulation.</li>
</ol>
<p>$$E &#x3D; { ( {job} _ i, {job} _ j ) \mid {job} _ i,{job} _ j \in V\ \text{and}\ {NP} _ {job _ x} &gt; {QoS} _ {job _ x} \times (1 + \delta ), x \in {i, j} }$$</p>
<ol start="4">
<li><strong>Execution</strong>. The batch of jobs are assigned to the modified GPU hardware.</li>
</ol>
<h2 id="Evaluations"><a href="#Evaluations" class="headerlink" title="Evaluations"></a>Evaluations</h2><p>The paper compare GPUPool against three baseline systems:</p>
<ol>
<li><p>No-Sharing.</p>
</li>
<li><p>Coarse: packing the jobs onto <strong>as few GPUs as possible</strong> using a greedy scheduling algorithm.</p>
</li>
<li><p>Heuristic: pairing up jobs with the <strong>highest and lowest bandwidth utilization</strong> (profiled offline) from a batch of incoming jobs.</p>
</li>
</ol>
<p>The metrics is system throughput $STP&#x3D;\sum_{i&#x3D;1}^n \cfrac{t_{isolated}^i}{t_{shared}^i}$. $t_{isolated}^i$ and $t_{shared}^i$ are turnaround time of the i-th concurrent job when executing in an isolated and shared environment respectively. The paper also uses we use ${QoS}_{reached}$ to evaluate QoS fulfilment rate.</p>
<p><img src="/2024-02-07-paper-reading-pact22-gpupool/gpu_sharing_compare.png" alt="Comparison of GPU Sharing Systems"></p>
<p><img src="/2024-02-07-paper-reading-pact22-gpupool/sorted_stp.png" alt="Sorted STP on GPUs"></p>
<p><img src="/2024-02-07-paper-reading-pact22-gpupool/throughput.png" alt="Throughput Normalized to QoS Target"></p>
<p><img src="/2024-02-07-paper-reading-pact22-gpupool/ml_pred.png" alt="Prediction Accuracy of Different ML Techniques"></p>
<h2 id="Comments"><a href="#Comments" class="headerlink" title="Comments"></a>Comments</h2><h3 id="Strengths"><a href="#Strengths" class="headerlink" title="Strengths"></a>Strengths</h3><p>This paper targets the fine-grained GPU sharing problem in the cloud. I believe this work provides a valuable solution to this problem.</p>
<p>From my perspective, fine-grained GPU sharing presents three key challenges:</p>
<ol>
<li><p><strong>Limitations imposed by hardware and CUDA</strong>, which make it difficult for programmers to flexibly control kernel execution.</p>
</li>
<li><p><strong>Reliable and low-cost performance prediction</strong> for concurrent kernel execution. Establishing an analytical performance prediction model is highly challenging. One naive approach is using real hardware to profile, but due to the $\mathcal{O}(n^2)$ ($n$ representing the number of jobs) time complexity, this method is not scalable to larger clusters.</p>
</li>
<li><p><strong>Efficient algorithms to find appropriate job combinations</strong>. If we allow an arbitrary number of jobs to execute concurrently, this becomes an NP-hard problem.</p>
</li>
</ol>
<p>This paper cleverly addresses or bypasses these challenges through the following strategies:</p>
<ol>
<li><p><strong>Hardware-software co-design</strong>, which involves modifying hardware to provide more flexible API for upper-layer application. While this prevents the authors from testing their method on actual hardware and forces them perform experiments on simulator (GPGPU-Sim), I believe such simulations can provide valuable insights for adjustments on real hardware.</p>
</li>
<li><p>Predicting kernel concurrent execution performance <strong>by a ML model</strong>. This is <strong>a standout aspect</strong> of the paper (which is also my <strong>favorite novelty</strong>). The authors introducing ML with a good motivation to effectively addresses a challenging performance modeling problem, bypassing a complicated analytical modeling. Also, this ML model has good <strong>interpretability</strong>, top-10 import metrics (show in Figure) align well with human’s intuition. Furthermore, in my research experiences about Deep Learning Compiler (e.g., TVM), I also found many paper introduce such ML models for performance prediction. I believe the thought that <strong>leveraging ML techniques to bypass some complicated modeling problems</strong> is highly valuable in system research, which is the most important thing I learned from this paper.</p>
</li>
<li><p>Instead of solving the whole NP-hard job combination problem, the authors limit the number of concurrently executed jobs to 2, considering this simpler case. It is <strong>a fantastic tradeoff</strong>. The simplified problem can be solved by a maximum cardinality matching algorithm, which may not find the optimal combination, but exchanging reasonable scheduling overhead for a substantial performance improvement.</p>
</li>
</ol>
<h3 id="Weaknesses"><a href="#Weaknesses" class="headerlink" title="Weaknesses"></a>Weaknesses</h3><p>This paper also has some potential weaknesses:</p>
<ol>
<li><p>It seems to ignore the situation which <strong>two concurrent jobs have different execution times</strong>. For instance, when a longer job and a shorter job are executed together, after the shorter job finishes, GPUPool seems unable to schedule a new job to the GPU. Instead, the remaining GPU time is monopolized by the longer job. This could result in a lower resource utilization.</p>
</li>
<li><p>The concurrent execution of multiple jobs on a single GPU may also be <strong>constrained by GPU memory capacity</strong>. A possible improvement is to ask users to indicate maximum GPU memory usage of their applications and consider the these constraints when constructing the graphs.</p>
</li>
<li><p>This paper does not consider <strong>the job which leverages multiple GPUs</strong>. These jobs are quite common in reality. When a job can occupy multiple GPUs, there are some additional constraints:</p>
<ol>
<li><p><strong>Inter-GPU connection</strong> (e.g., NVLink or InfiniBand) bandwidth is the potential bottleneck, especially for distributed training strategies relying on high GPU interconnect bandwidth, such as <em>Data Parallelism</em>. Improper job scheduling may lead to contention for bandwidth among multiple jobs, or jobs requiring high GPU interconnect bandwidth may run on different nodes.</p>
</li>
<li><p>When a single job leverages multiple GPUs, <strong>the workload types on different GPUs may not be the same</strong>. For example, in <em>Pipeline Parallelism</em>, different GPUs run different stages of the neural network.</p>
</li>
</ol>
</li>
<li><p>This paper does not clearly take into account <strong>the impact of memory hierarchy on performance</strong>, such as shared memory (or just implicitly consider it using a ML model). Some CUDA kernels are optimized by carefully utilizing CUDA SM shared memory, such as <em>Flash Attention</em>. When two kernels run together, does it lead to shared memory contention? Could it result in runtime errors or shared memory overflowing into global memory, causing a severe performance decline? Experiments in the paper can not answer these questions. Also, the selected profiling metrics to train stage 1 model listed in Figure 5 do not contains any metrics about shared memory capacity. Another possibility is that a ML model is already good enough to handle this problem. Regardless, the impact of memory hierarchy on GPU-sharing deserves further study.</p>
</li>
</ol>
<p><img src="/2024-02-07-paper-reading-pact22-gpupool/metrics.png" alt="Figure 5. Metrics Used to Train Stage 1 Prediction Model"></p>
<h3 id="Possible-Improvements"><a href="#Possible-Improvements" class="headerlink" title="Possible Improvements"></a>Possible Improvements</h3><p>I have some potential ideas to improve this work:</p>
<ol>
<li><p>As response to the first weakness mentioned above, we can extend GPUPool to enable it to schedule a new job to the GPU after the shorter job finishes. This improvement can be achieved by a simple modification: <strong>keep the running jobs in the incoming window, and if two jobs are still running in the same GPU, also keep the edge between them in the pairing graph</strong>. With this modification, if shorter job finishes, we can re-run the matching algorithm to find a new job to pair with it.</p>
</li>
<li><p>We can extend GPUPool to support <strong>multiple GPU job</strong>. To achieve that, we should consider inter-GPU connection bandwidth. This may include following modifications:</p>
<ol>
<li><p>Ask users to <strong>indicate the required inter-GPU bandwidth or connection types</strong> (e.g., NVLink&#x2F;PCIe&#x2F;Infiniband&#x2F;Ethernet).</p>
</li>
<li><p>Take a multiple GPU task as several sub-jobs. <strong>Each of sub-job is a single GPU job</strong>, with interconnection constraints. Then we can reuse the infrastructure of GPUPool to find the co-running chances.</p>
</li>
<li><p>Extend the last <strong>step “Execution” to consider the interconnection constraints</strong>, so it can dispatch sub-jobs to nodes that meet the constraints. This may require an efficient graph algorithm to find job placement, which requires a further research.</p>
</li>
</ol>
</li>
<li><p>Sometimes the goal of a data center is not just to improve resource utilization, but also to <strong>save energy</strong>. Improving resource utilization does not necessarily mean energy saving, because the chip’s speed $S$, power consumption $P$, and frequency $f$ have the following approximate relationship:</p>
</li>
</ol>
<p>$$S \propto f$$<br>$$P \propto f^\alpha, \text{while}\ \alpha \in [2, 3]$$</p>
<p>We can extend the optimization target of GPUPool to power consumption. This can be achieved by add a power prediction model with similar methods. Then we can use a multi-objective optimization algorithm to find the best job combination, considering both performance and power consumption.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://monsoon-cs.moe/2024-01-29-wg-for-cluster/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Monsoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024-01-29-wg-for-cluster/" class="post-title-link" itemprop="url">Building WireGuard VPN for Machine Learning Server Cluster</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-01-29 00:00:00" itemprop="dateCreated datePublished" datetime="2024-01-29T00:00:00+08:00">2024-01-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-07 04:33:26" itemprop="dateModified" datetime="2024-02-07T04:33:26+08:00">2024-02-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>机器学习集群需要一个安全的方式向用户暴露服务，以及跨公网服务器互联，为此需要部署 VPN 网络。</p>
<p>VPN 网络的部署需要考虑如下因素：</p>
<ol>
<li>网络拓扑：需要选择合适的拓扑结构以尽可能降低延迟；</li>
<li>用户管理：可以方便地进行用户的增减和授权；</li>
<li>使用和维护简单。</li>
</ol>
<h2 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h2><h3 id="网络拓扑"><a href="#网络拓扑" class="headerlink" title="网络拓扑"></a>网络拓扑</h3><p>网络拓扑决定着延迟。</p>
<p>延迟最低的方案显然是 full-mesh，即每一对 peer 之间都有直接的 P2P 连接。但这种拓扑结构的管理复杂度是 $\mathcal{O}(n^2)$ 的，并且每添加一个新的 peer 就需要修改所有其他 peer 的配置文件，还需要解决 NAT 带来的问题，这必须借助一些自动化的软件管理。我尝试了 <a target="_blank" rel="noopener" href="https://www.netmaker.io/">Netmaker</a> 和 <a target="_blank" rel="noopener" href="https://headscale.net/">Headscale</a>，但它们似乎都无法正确处理学校内的<strong>复杂网络环境</strong>，比如各种企业级路由器使用的 symmetric NAT，<strong>成功建立 P2P 的概率非常之低</strong>。</p>
<p>最终我选择了 <strong>full-mesh 和 hub-and-spoke 相结合的拓扑</strong>。由于服务器数量和 IP 很少变化，手动配置一个服务器间的 full-mesh 网络是可行的。与此同时，提供一个 gateway server 作为用户接入的 hub，用户只需要与 gateway server 建立连接。由于大部分用户其实是在校内使用 VPN 的，因此连接到校内的 gateway server 并转发流量并不会带来太多额外延迟。这种结构可以平衡延迟与管理复杂度，用户的增减和授权也只需要在 gateway server 上操作。</p>
<p><img src="/2024-01-29-wg-for-cluster/topo.png" alt="Network Topology"></p>
<h3 id="协议选择"><a href="#协议选择" class="headerlink" title="协议选择"></a>协议选择</h3><p>流行的 OpenVPN 和 IPSec 都足够优秀，但新兴的 WireGuard 具有无可比拟的配置简单性。对于服务端，WireGuard 可以用几行配置文件定义一个 peer 和路由；对于用户，由于 WireGuard 采用基于密钥对的认证方式，只需要一个配置文件即可接入 VPN 网络，不需要额外的密码记忆和登录操作。</p>
<h3 id="管理方式"><a href="#管理方式" class="headerlink" title="管理方式"></a>管理方式</h3><p>出于可预测性和稳定性的考量，我选择了手动配置的方法。服务器间的 full-mesh 网络一次配置后就不需要再频繁更改。而用户管理则通过一个脚本实现，当需要添加一个新用户时，脚本生成密钥对并分配 IP，把公钥和路由信息加入 gateway server 的 peer list 中，然后生成包含私钥和分配的 IP 的配置文件，并发给用户。</p>
<p>Gateway server 上的用户 peer 配置示例：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[Peer]</span></span><br><span class="line"><span class="attr">PublicKey</span> = &lt;redacted&gt;</span><br><span class="line"><span class="attr">AllowedIPs</span> = <span class="number">10.1</span>.x.y/<span class="number">32</span></span><br><span class="line"><span class="attr">AllowedIPs</span> = fd01::x:y/<span class="number">128</span></span><br><span class="line"><span class="attr">PersistentKeepalive</span> = <span class="number">25</span></span><br></pre></td></tr></table></figure>

<p>用户的接入配置文件示例：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[Interface]</span></span><br><span class="line"><span class="attr">PrivateKey</span> = &lt;redacted&gt;</span><br><span class="line"><span class="attr">Address</span> = <span class="number">10.1</span>.x.y/<span class="number">16</span></span><br><span class="line"><span class="attr">Address</span> = fd01::x:y/<span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="section">[Peer]</span></span><br><span class="line"><span class="attr">PublicKey</span> = &lt;redacted&gt;</span><br><span class="line"><span class="attr">AllowedIPs</span> = <span class="number">10.1</span>.<span class="number">0.0</span>/<span class="number">16</span>  <span class="comment"># route all VPN traffic to gateway server</span></span><br><span class="line"><span class="attr">AllowedIPs</span> = fd01::/<span class="number">64</span></span><br><span class="line"><span class="attr">Endpoint</span> = wg.ustcaigroup.xyz:<span class="number">51820</span>  <span class="comment"># gateway server is dual stack</span></span><br><span class="line"><span class="comment"># Endpoint = wg.ustcaigroup.xyz:51820  # IPv4</span></span><br><span class="line"><span class="comment"># Endpoint = wg.ustcaigroup.xyz:51820  # IPv6</span></span><br><span class="line"><span class="attr">PersistentKeepalive</span> = <span class="number">25</span></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://monsoon-cs.moe/2023-11-24-storage-system-desgin/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Monsoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023-11-24-storage-system-desgin/" class="post-title-link" itemprop="url">Building Storage System for Machine Learning Server Cluster</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-11-24 00:00:00" itemprop="dateCreated datePublished" datetime="2023-11-24T00:00:00+08:00">2023-11-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-07 04:33:26" itemprop="dateModified" datetime="2024-02-07T04:33:26+08:00">2024-02-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>This is an unfinished blog.</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://monsoon-cs.moe/2023-11-14-ascend-910b-custom-op/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Monsoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023-11-14-ascend-910b-custom-op/" class="post-title-link" itemprop="url">Ascend 910B 自定义 PyTorch 算子</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-11-14 00:00:00" itemprop="dateCreated datePublished" datetime="2023-11-14T00:00:00+08:00">2023-11-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-07 04:33:26" itemprop="dateModified" datetime="2024-02-07T04:33:26+08:00">2024-02-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>本文基于的硬件环境为 Ascend 910B3，基于的软件环境包括 <a target="_blank" rel="noopener" href="https://www.hiascend.com/developer/download/community/result">CANN 7.0-RC1</a>、<a target="_blank" rel="noopener" href="https://repo.huaweicloud.com/kunpeng/archive/Ascend/PyTorch/">PyTorch 1.11.0</a>、<a target="_blank" rel="noopener" href="https://gitee.com/ascend/pytorch/releases/tag/v5.0.rc3-pytorch1.11.0">Ascend PyTorch Adapter v5.0.rc3-pytorch1.11.0</a>。其他 CANN 和 PyTorch 版本上的情况可能略有不同。</p>
<h2 id="注册过程"><a href="#注册过程" class="headerlink" title="注册过程"></a>注册过程</h2><h3 id="Ascend-PyTorch-Adapter-中添加自定义算子"><a href="#Ascend-PyTorch-Adapter-中添加自定义算子" class="headerlink" title="Ascend PyTorch Adapter 中添加自定义算子"></a>Ascend PyTorch Adapter 中添加自定义算子</h3><blockquote>
<p>参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.hiascend.com/document/detail/zh/canncommercial/70RC1/operatordev/Ascendcopdevg/atlas_ascendc_10_0045.html">https://www.hiascend.com/document/detail/zh/canncommercial/70RC1/operatordev/Ascendcopdevg/atlas_ascendc_10_0045.html</a></li>
<li><a target="_blank" rel="noopener" href="https://gitee.com/ascend/samples/tree/master/operator/AddCustomSample/FrameworkLaunch/PytorchInvocation">https://gitee.com/ascend/samples/tree/master/operator/AddCustomSample/FrameworkLaunch/PytorchInvocation</a></li>
</ul>
</blockquote>
<p>在 <code>torch_npu/csrc/aten/npu_native_functions.yaml</code> 中添加 <code>npu_add_custom</code> 函数：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">custom:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">func:</span> <span class="string">npu_add_custom(Tensor</span> <span class="string">x,</span> <span class="string">Tensor</span> <span class="string">y)</span> <span class="string">-&gt;</span> <span class="string">Tensor</span>  <span class="comment"># 添加的函数</span></span><br></pre></td></tr></table></figure>

<p>在 <code>torch_npu/csrc/aten/ops/op_api</code> 中添加 <code>AddCustomKernelNpu.cpp</code> 文件：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/csrc/autograd/custom_function.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;torch_npu/csrc/framework/utils/OpAdapter.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;torch_npu/csrc/aten/NPUNativeFunctions.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;torch_npu/csrc/aten/ops/op_api/op_api_common.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> at_npu &#123;</span><br><span class="line">  <span class="keyword">namespace</span> native &#123;</span><br><span class="line">    <span class="keyword">using</span> torch::autograd::Function;</span><br><span class="line">    <span class="keyword">using</span> torch::autograd::AutogradContext;</span><br><span class="line"></span><br><span class="line">    <span class="function">at::Tensor <span class="title">NPUNativeFunctions::npu_add_custom</span><span class="params">(<span class="type">const</span> at::Tensor&amp; x, <span class="type">const</span> at::Tensor&amp; y)</span> </span>&#123;</span><br><span class="line">        at::Tensor result = OpPreparation::<span class="built_in">ApplyTensor</span>(x); <span class="comment">// 创建输出内存</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// calculate the output result of the NPU</span></span><br><span class="line">        <span class="built_in">EXEC_NPU_CMD</span>(aclnnAddCustom, x, y, result);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="comment">// namespace native</span></span><br><span class="line">&#125; <span class="comment">// namespace at_npu</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>之后重新编译安装 <code>torch_npu</code>。</p>
<h3 id="CANN-中添加自定义算子的实现"><a href="#CANN-中添加自定义算子的实现" class="headerlink" title="CANN 中添加自定义算子的实现"></a>CANN 中添加自定义算子的实现</h3><blockquote>
<p>参考：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.hiascend.com/document/detail/zh/canncommercial/70RC1/operatordev/Ascendcopdevg/atlas_ascendc_10_0023.html">https://www.hiascend.com/document/detail/zh/canncommercial/70RC1/operatordev/Ascendcopdevg/atlas_ascendc_10_0023.html</a></li>
</ul>
</blockquote>
<p>首先定义算子描述文件 <code>add_custom.json</code>：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;op&quot;</span><span class="punctuation">:</span> <span class="string">&quot;AddCustom&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;language&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cpp&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;input_desc&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;x&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;param_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;required&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;format&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                    <span class="string">&quot;ND&quot;</span></span><br><span class="line">                <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                    <span class="string">&quot;fp16&quot;</span></span><br><span class="line">                <span class="punctuation">]</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;y&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;param_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;required&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;format&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                    <span class="string">&quot;ND&quot;</span></span><br><span class="line">                <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                    <span class="string">&quot;fp16&quot;</span></span><br><span class="line">                <span class="punctuation">]</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;output_desc&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;z&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;param_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;required&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;format&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                    <span class="string">&quot;ND&quot;</span></span><br><span class="line">                <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                    <span class="string">&quot;fp16&quot;</span></span><br><span class="line">                <span class="punctuation">]</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>

<p>执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">msopgen gen -i add_custom.json -c ai_core-Ascend910B3 -f pytorch -out . -lan cpp</span><br></pre></td></tr></table></figure>

<p>生成算子工程：</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">AddCustom</span><br><span class="line">├── build.sh</span><br><span class="line">├── cmake </span><br><span class="line">│   ├── config.cmake</span><br><span class="line">│   ├── func.cmake</span><br><span class="line">│   ├── intf.cmake</span><br><span class="line">│   ├── makeself.cmake</span><br><span class="line">│   └── util</span><br><span class="line">├── CMakeLists.txt</span><br><span class="line">├── CMakePresets.json          // 修改 ASCEND_CANN_PACKAGE_PATH</span><br><span class="line">├── framework</span><br><span class="line">├── op_host</span><br><span class="line">│   ├── add_custom_tiling.h    // 定义 length 和 tiling 相关信息</span><br><span class="line">│   ├── add_custom.cpp         // 算子 host 侧实现</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">├── op_kernel</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   ├── add_custom.cpp         // 算子 kernel 侧实现</span><br><span class="line">└── scripts</span><br></pre></td></tr></table></figure>

<p><code>CMakePresets.json</code> 中修改 <code>ASCEND_CANN_PACKAGE_PATH</code> 为 CANN 安装路径。</p>
<p><code>op_host/add_custom_tiling.h</code> 的内容如下（简单实现）：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;register/tilingdata_base.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> optiling &#123;</span><br><span class="line"><span class="built_in">BEGIN_TILING_DATA_DEF</span>(AddCustomTilingData)</span><br><span class="line">    <span class="built_in">TILING_DATA_FIELD_DEF</span>(<span class="type">uint32_t</span>, size);  <span class="comment">// 定义 tensor size</span></span><br><span class="line">END_TILING_DATA_DEF;</span><br><span class="line"></span><br><span class="line"><span class="built_in">REGISTER_TILING_DATA_CLASS</span>(AddCustom, AddCustomTilingData)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>op_host/add_custom.cpp</code> 中修改算子调用时的 <code>block_dim</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context-&gt;<span class="built_in">SetBlockDim</span>(<span class="number">20</span>); <span class="comment">// 910B3 的 block_dim</span></span><br></pre></td></tr></table></figure>

<p><code>op_kernel/add_custom.cpp</code> 是算子的具体实现：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;kernel_operator.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> __DAV_C220_VEC__</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">extern</span> <span class="string">&quot;C&quot;</span> <span class="function">__global__ __aicore__ <span class="type">void</span> <span class="title">add_custom</span><span class="params">(GM_ADDR x, GM_ADDR y, GM_ADDR z, GM_ADDR workspace, GM_ADDR tiling)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">GET_TILING_DATA</span>(tiling_data, tiling);</span><br><span class="line">    <span class="type">uint32_t</span> M = tiling_data.size;  <span class="comment">// 从 tiling_data 中获取 tensor size</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 重要：CANN 会尝试不同的 ccec 编译参数以推断算子的类型（VEC、CUBE、MIXED），如果不创建一个 stub 函数将会编译失败</span></span><br><span class="line"><span class="keyword">extern</span> <span class="string">&quot;C&quot;</span> <span class="function">__global__ __aicore__ <span class="type">void</span> <span class="title">add_custom</span><span class="params">(GM_ADDR x, GM_ADDR y, GM_ADDR z, GM_ADDR workspace, GM_ADDR tiling)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">pip_barrier</span>(PIPE_ALL);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br></pre></td></tr></table></figure>

<h3 id="编译部署"><a href="#编译部署" class="headerlink" title="编译部署"></a>编译部署</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">bash build.sh</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">./custom_opp_euleros_aarch64.run</span></span><br></pre></td></tr></table></figure>

<p>PyTorch 中调用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch_npu</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">z = torch.npu_add_custom(x, y)  <span class="comment"># 由于是运行时编译，第一次运行时需要等待编译</span></span><br></pre></td></tr></table></figure>

<h2 id="注册原理"><a href="#注册原理" class="headerlink" title="注册原理"></a>注册原理</h2><p>TODO</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>TODO</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://monsoon-cs.moe/2023-11-09-proxy-for-team/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Monsoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023-11-09-proxy-for-team/" class="post-title-link" itemprop="url">Building Proxy Service for Team</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-11-09 00:00:00" itemprop="dateCreated datePublished" datetime="2023-11-09T00:00:00+08:00">2023-11-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-07 04:33:26" itemprop="dateModified" datetime="2024-02-07T04:33:26+08:00">2024-02-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>This is an unfinished blog.</p>
</blockquote>
<h2 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h2><p>Due to <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Internet_censorship_in_China">Internet censorship in China</a> (known as <em>GFW</em>, <em>Great Firewall</em>, <em>防火长城</em>), many websites (e.g. Google, Twitter) are blocked, and some websites (e.g. GitHub) suffer connectivity issues. In China, the means to circumvent internet censorship is referred to as <em>翻墙</em> (means <em>climbing over the wall</em>).</p>
<p>In China, to freely access the Internet, a proxy is essential. Despite various commercial options available, they may not be suitable for everyone. Therefore, I have constructed a user-friendly and easy-to-maintain proxy system for my research group, as a part of my responsibilities as a system administrator.</p>
<h2 id="Target"><a href="#Target" class="headerlink" title="Target"></a>Target</h2><ol>
<li><strong>Easy to use</strong>. Team members only need some simple configurations.The proxy client should be able to automatically update configuration.</li>
<li><strong>Stability</strong>.</li>
<li><strong>Sufficient traffic</strong>, to download large datasets.</li>
<li><strong>Low Latency</strong>, to provide good experience for web.</li>
<li><strong>Low Cost</strong>.</li>
<li><strong>Easy to maintain</strong>. Frequent maintenance is unacceptable, and only simple changes of the configuration are required for new function.</li>
<li><strong>Concealment</strong>. The cat-and-mouse game between GFW and anti-censorship tools has been escalating. Ten years ago (2013), only an OpenVPN client was all your need to <a target="_blank" rel="noopener" href="https://www.cnnic.com.cn/IDR/hlwfzdsj/201306/t20130628_40563.htm">“Across the Great Wall and reach every corner in the world”</a>. Now, you must use much more sophisticated solutions to prevent your “unusual” traffic from being detected by GFW. According to <a target="_blank" rel="noopener" href="https://gfw.report/">GFW Report</a>, popular <a target="_blank" rel="noopener" href="https://shadowsocks.org/">Shadowsocks</a> (a proxy protocol which simply encrypt all traffic using pre-shared key) was <a target="_blank" rel="noopener" href="https://gfw.report/blog/gfw_shadowsocks/">detected and blocked</a>, and the TLS-based proxy also <a target="_blank" rel="noopener" href="https://github.com/net4people/bbs/issues/129">encountered large-scale blocking in Oct 2022</a>. The tools and protocols used must be concealed enough to allow the service to run for a long time.</li>
</ol>
<h2 id="Available-Resources"><a href="#Available-Resources" class="headerlink" title="Available Resources"></a>Available Resources</h2><h3 id="CERNET"><a href="#CERNET" class="headerlink" title="CERNET"></a>CERNET</h3><h3 id="Cloudflare-WARP"><a href="#Cloudflare-WARP" class="headerlink" title="Cloudflare WARP"></a>Cloudflare WARP</h3><h3 id="VPS"><a href="#VPS" class="headerlink" title="VPS"></a>VPS</h3><h3 id="Server-in-USTC"><a href="#Server-in-USTC" class="headerlink" title="Server in USTC"></a>Server in USTC</h3><h3 id="Anti-Censorship-Tools"><a href="#Anti-Censorship-Tools" class="headerlink" title="Anti-Censorship Tools"></a>Anti-Censorship Tools</h3><h2 id="Adopted-Solution"><a href="#Adopted-Solution" class="headerlink" title="Adopted Solution"></a>Adopted Solution</h2><!-- draw a picture -->

<h2 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h2><h2 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h2><h3 id="Client-Initialization"><a href="#Client-Initialization" class="headerlink" title="Client Initialization"></a>Client Initialization</h3><h3 id="Compatibility"><a href="#Compatibility" class="headerlink" title="Compatibility"></a>Compatibility</h3><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://monsoon-cs.moe/2023-11-05-toefl-exp/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Monsoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023-11-05-toefl-exp/" class="post-title-link" itemprop="url">我的 TOEFL 经验</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-11-05 00:00:00" itemprop="dateCreated datePublished" datetime="2023-11-05T00:00:00+08:00">2023-11-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-07 04:33:26" itemprop="dateModified" datetime="2024-02-07T04:33:26+08:00">2024-02-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>作为高考以来带给我最大焦虑感的考试，TOEFL 让我 2023 年大部分时间在黑暗中度过，我对其的时间、金钱投入也是最大的。</p>
<p>一开始定下总分 100、口语 20 的目标，中间经历了无数天自信心丧失、被焦虑情绪淹没、口语练到舌头打结，最终在 2023 年 11 月 3 日查询到了满意的成绩。</p>
<p>我写下此文既作为自己过去的总结，也希望能帮助到可能看到这篇文章的人。</p>
<p>我参加的场次和得分情况：</p>
<table>
<thead>
<tr>
<th align="center">考试时间</th>
<th align="center">总分</th>
<th align="center">阅读</th>
<th align="center">听力</th>
<th align="center">口语</th>
<th align="center">写作</th>
<th align="center">备注</th>
</tr>
</thead>
<tbody><tr>
<td align="center">2023.7.22</td>
<td align="center">89</td>
<td align="center">27</td>
<td align="center">24</td>
<td align="center">16</td>
<td align="center">22</td>
<td align="center">改革前</td>
</tr>
<tr>
<td align="center">2023.8.15</td>
<td align="center">89</td>
<td align="center">28</td>
<td align="center">25</td>
<td align="center">17</td>
<td align="center">19</td>
<td align="center">这场起为改革后</td>
</tr>
<tr>
<td align="center">2023.9.16</td>
<td align="center">96</td>
<td align="center">29</td>
<td align="center">27</td>
<td align="center">19</td>
<td align="center">21</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">2023.10.14</td>
<td align="center">96</td>
<td align="center">30</td>
<td align="center">24</td>
<td align="center">19</td>
<td align="center">23</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">2023.10.28</td>
<td align="center">101</td>
<td align="center">28</td>
<td align="center">27</td>
<td align="center">22</td>
<td align="center">24</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">MyBest</td>
<td align="center">103</td>
<td align="center">30</td>
<td align="center">27</td>
<td align="center">22</td>
<td align="center">24</td>
<td align="center"></td>
</tr>
</tbody></table>
<p>用到的学习材料：</p>
<ul>
<li>背单词：<a target="_blank" rel="noopener" href="https://www.maimemo.com/">墨墨背单词</a></li>
<li>听力口语训练：<a target="_blank" rel="noopener" href="https://toefl.kmf.com/">学而思考满分</a>、<a target="_blank" rel="noopener" href="https://tpo.xdf.cn/">新东方 TOEFL</a>、某宝买的 TPO 1~74 所有口语题目</li>
<li>口语参考：<a target="_blank" rel="noopener" href="https://book.douban.com/subject/30300871/">新东方《托福口语白皮书》</a></li>
<li>写作参考：<a target="_blank" rel="noopener" href="https://book.douban.com/subject/26338897/">新东方《托福写作白皮书》</a>，改革后<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/648415673">所有学术交流写作真题及范文</a></li>
</ul>
<h2 id="阅读"><a href="#阅读" class="headerlink" title="阅读"></a>阅读</h2><p>对于大部分中国学生而言这是最简单的部分，一个合格的 211 以上的学生必定能轻松应对。</p>
<p>我在考前只做了两篇适应一下做题节奏，第一次考就取得了 27 分，之后一直稳定，并在第四次考到了满分。个人感觉 TOEFL 阅读难度甚至低于江苏高考和六级阅读。虽然我第一次考试前背了许多单词，但那更多是为了 GRE 准备的，TOEFL 阅读本身基本无词汇方面的挑战。</p>
<p>虽然高分并不难，但满分还需要一点运气。我考满分的那次，两篇阅读的话题分别是「地球早期的海洋与大气」、「农业革命与灌溉」，都是我非常熟悉的话题。这种情况下的阅读就是简单模式。</p>
<h2 id="听力"><a href="#听力" class="headerlink" title="听力"></a>听力</h2><p>TOEFL 奇葩的考试模式，让听力、口语、写作都考察你的听力能力。但<strong>这三部分听力其实是完全不一样的</strong>：</p>
<ul>
<li>听力部分本身：<ul>
<li>Conversation：难度较高，日常对话一直是我薄弱的地方，连读、吞音等现象最多，语速也较快；</li>
<li>Lecture：难度一般，虽然看似很长，但其实语速较慢，容错也高，一句话没听清完全可以根据上下文 infer；</li>
</ul>
</li>
<li>综合口语：这部分的听力其实难度最高，需要尽量多捕获细节并记下充分的笔记，我口语基础本身很差，难上加难；</li>
<li>综合写作：难度最低，一开始会让你读完一篇阅读材料熟悉话题，并且听力结构死板，逻辑清晰，语速较慢。</li>
</ul>
<p>但不得不说，<strong>经过恰当的训练，听力部分也是很容易提分、考到高分的。</strong>我集中大量训练了 20 天左右，另外还有大概 30 天零零碎碎的训练（和别的事情混在一起）。</p>
<p>关于听力，最重要的一点是，<strong>必须要摸索出适合你自己的做题方式。</strong>很多学习资料会强调听力时如何正确记笔记，我一开始也是那样训练的，但考完第一次后我发现这种方法并不适合我，记笔记会分散你的注意力，听力内容跟丢（不再能把握上下文的逻辑关系）的概率会极大增加。</p>
<p>我的总结是，<strong>笔记适合记录细节，人脑适合记住逻辑。</strong></p>
<p><strong>TOEFL 纯听力部分其实并不注重细节，反而更考察你对听力材料的整体把握。</strong>后来我 20 天的专门训练中，我就彻底抛弃了笔记，效果很好。需要说明的是，后来我发现遇到细节密度比较高的时候，偶尔记笔记还是有用的，能帮助你避免走神，记下的内容其实没用，我考场上从没看过。在这里，记笔记其实只是为了强化人脑记忆，并不是一种外部信息存储方式。</p>
<p>我自己使用的听力训练法：第一遍做题，第二遍重听，第三遍看着文字内容听，之后再听若干次，直到你能听清每个细节为止。专门训练时每篇听力我大概要花 20~40 分钟不等，每天练至少 6 篇。</p>
<p>同样，话题熟悉度会很大程度上影响发挥。我第一次得 27 分的那场，有篇 lecture 是讲的经典故事「胶带手撕石墨烯得诺贝尔奖」，虽然我很熟悉并且做得顺风顺水，但内容确实有点偏专业，有许多物理学专业词汇，涉及石墨烯的分层结构、各向异性的导电性的原理。由于 TOEFL 听力 lecture 还是以理工科为主，摸鱼时从知乎 B 站上学到的没用知识，甚至中学时代看过的一些科普读物，都可能以一种意想不到的方式帮助你，广博的知识面会让你事半功倍。但与此同时，遇到不熟悉的话题就很麻烦，我第四场考试听力只得 24 分，原因就是遇到了一个 literature 话题，大部分内容没听明白。</p>
<p><strong>2023 年 7 月改版后</strong>听力有个坑点：由于取消了中场休息，有些人做得快，会在你听听力时就开始讲口语，产生严重干扰。第二场考试前虽然我专项训练了，但听力仍然只有 25 分，就是踩了这个坑。</p>
<p>避免此坑的方法是，所有的 direction 部分全部快速跳过，阅读部分可以剩两分钟提前结束，这样你可以成为全场第一个讲口语的人，<del>让别人被你干扰</del>。</p>
<blockquote>
<p><del>宁叫我负天下人，不叫天下人负我。</del></p>
</blockquote>
<h2 id="口语"><a href="#口语" class="headerlink" title="口语"></a>口语</h2><p>看分数就知道这是最折磨我的一部分，甚至后两场就只是为口语考的（口语没到 20 申请时非常危险）。</p>
<p>口语专项我高强度训练大概 30 天，非专项训练的天数加起来数不清了。</p>
<p>对于像我这种口语基础很差的人来说，大量的训练可以保证你的分数能在 20 左右，之后还是要看运气和临场发挥。</p>
<p><strong>TOEFL 口语与其说是口语考试，不如说是大综合</strong>。对我个人而言，口语部分的阅读和听力要求甚至高于阅读和听力部分本身：</p>
<ul>
<li>task2、task3 的阅读部分要求<strong>速读能力</strong>，个人感觉没有 4 words&#x2F;s 是搞不定的，而且你<strong>不会有没读通后回滚的机会</strong>。而阅读部分其实完全可以照我平时看论文的速度去看，一句话没看明白也能多看几遍。</li>
<li>综合口语的听力要求你记下细节，相比之下听力部分很多时候只要记下逻辑就行。记细节就必须依赖笔记，平衡好笔记、接收信息和把握整体逻辑，是最为困难的。</li>
</ul>
<h3 id="独立口语"><a href="#独立口语" class="headerlink" title="独立口语"></a>独立口语</h3><p>素材积累是有必要的，但数量不在多，我只准备了 10 个常用的，重要的是一定要熟练运用，看到题目需要快速反应出来可以套什么素材。这可以去专门练习学而思考满分上的<a target="_blank" rel="noopener" href="https://toefl.kmf.com/speak/gold/1">口语黄金 80 题</a>。</p>
<p>同时素材也不是万能的，独立口语不可避免地带有许多随机因素，经常需要临场发挥编故事，这时用中文快速想好后翻译成英文（写下几个关键词，说的时候连词成句）会比较快。</p>
<h3 id="综合口语"><a href="#综合口语" class="headerlink" title="综合口语"></a>综合口语</h3><p>对我来说整场考试难度最高部分，每次考到这基本就肾上腺素爆发。</p>
<p><strong>如何应对综合口语是我花最多时间训练的部分，没有什么捷径，必须要自己找感觉、找经验。</strong>我在这里说一下我总结出的适合我的经验：</p>
<ul>
<li>阅读时：task2、task3 虽然给了你 45s 阅读，<strong>但最好只用 15s 就扫完，并找出关键句（非关键句直接不看），之后把关键句抄下来</strong>（不必一字不差，但尽量完整，可以直接读，不必组织语言的那种）。这样做的好处是，我在准备时间可以直接快速读一遍，正式说的时候一开始不仅流畅而且节省时间；</li>
<li>听力时：尽可能记下细节，但必须要同时排除非重点，重点部分则同样记下关键词&#x2F;句。与此同时，记笔记绝对不能影响到接收信息本身；</li>
<li>准备时：一边把要说的内容读出来（不要默念，默念会让你产生你已经说流畅了的错觉），一边圈出有用的信息（或者划掉无用信息），用箭头整理出一条说的线，必要时在一些关键词间写下填充内容，降低临时组织语言的负担；</li>
<li>正式说：以保证流畅度为优先目标，时间不够了、卡住了时可以丢弃部分细节。结结巴巴、重复一句话不仅会降低分数，还会浪费时间。</li>
</ul>
<p><strong>无论什么情况下，千万不能过度紧张。</strong>过度紧张会让你思考变慢，也会极大增加说的时候的卡顿。我得 22 分的那场，考口语时就处于比较放松的状态。</p>
<p>综合口语我个人的训练方法：第一遍正常做，然后紧接着重说一遍，之后看解答，然后不停重复说直到能非常流畅。这种训练方式下一篇大概需要 15~30 分钟，我一天练 10 篇。</p>
<h2 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h2><p><strong>没有感情，全是套路。</strong>实际上我根本没有在写作训练上投入多少时间，一般的英语基础加上适当的技巧就能拿到至少 22。</p>
<p>需要注意的是，<strong>不要让打字速度拖你后腿</strong>。我是打字速度比较慢、并且 typo 很多的人，前两场次这确实影响到了我，不过后来熟练了也没问题了。</p>
<h3 id="综合写作"><a href="#综合写作" class="headerlink" title="综合写作"></a>综合写作</h3><p>综合写作的阅读可以定定心心读，给的时间甚至够你看两遍，也不用记笔记。听力部分也很简单，有阅读做铺垫让你熟悉话题，同时结构死板，逻辑清晰，语速较慢，记下重要细节并不困难。</p>
<p>要注意的是<strong>不要死背模版</strong>，把考试时间浪费在打模版上得不偿失，保证逻辑清晰结构工整即可。时间应该用在尽量多还原细节上，language use 用高考级别的词汇就行了，足够拿到 24 分。</p>
<h3 id="学术交流写作"><a href="#学术交流写作" class="headerlink" title="学术交流写作"></a>学术交流写作</h3><p>2023 年 7 月改革后去掉了独立写作，换成了学术交流写作，时间缩短到 10 分钟。第二次考试写作只有 19，是因为我心大完全没练新题型就上了，结果就是完全没按照要求答。</p>
<p>后来我花了半天专门训练了学术交流写作，基本上道。考试时其实只要看 professor 的提问，一堆废话不用看，之后扫一眼两个 student sample answers，找出核心观点，这是为了避免观点撞车，具体内容也不用看完，之后就可以开始写了。</p>
<p>我个人的模版如下：</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">From my perspective, &lt;我的观点&gt;.</span><br><span class="line"></span><br><span class="line">Although &lt;找一个你反对的 sample answer 抄上他的观点&gt;，&lt;简单说一下我的观点的 advantage&gt;.</span><br><span class="line"></span><br><span class="line">&lt;详细展开，可以用些例子，也可以指出你反对的观点的不足，60～70 words 足够&gt;.</span><br><span class="line"></span><br><span class="line">&lt;（可选，我个人喜欢的表达方式）有时候可以说一下我的 method 其实可以更好地达到我反对的 method 的目标&gt;.</span><br><span class="line"></span><br><span class="line">So, &lt;总结观点&gt;.</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><strong>不积跬步，无以至千里。</strong></p>
<p>对我个人而言，TOEFL 让我反思了大学以来的学习模式。本科时的课程要么是我已经熟悉的或者有基础的，要么是考前突击的。TOEFL 这种语言考试没有捷径（除非你是语言天才），必须从 Day 1 开始一点点训练，一点点找感觉、找经验。在这个过程种除了题目的障碍，更多还有负面情绪的障碍，找一些你信任的、同时愿意倾听你的人分享情绪非常有帮助。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://monsoon-cs.moe/2023-11-01-catching-mining-virus/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Monsoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023-11-01-catching-mining-virus/" class="post-title-link" itemprop="url">Catching Mining Virus</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-11-01 00:00:00" itemprop="dateCreated datePublished" datetime="2023-11-01T00:00:00+08:00">2023-11-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-07 04:33:26" itemprop="dateModified" datetime="2024-02-07T04:33:26+08:00">2024-02-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>On October 30, 2023, I received a warning message from the data center administrator, informing me that the firewall detected mining traffic sending from the server managed by me.</p>
<p><img src="/2023-11-01-catching-mining-virus/firewall_warning.png"></p>
<p>The “mining traffic” was a <code>bitcoin.sipa.be</code> DNS request sent to <code>223.5.5.5</code>.</p>
<p>Initially, I thought it was a simple task to find the virus process, just like my previous encounter with another mining virus. In that case, the hacker logged in the server by hacking a weak SSH password, gained root permission possibly by an privilege escalation vulnerability exploitation (it was a server running EOL Ubuntu 16.04). Then a cron job was set up to run a mining virus.</p>
<p>However, this time the situation was different. I couldn’t find any suspicious processes, and there was no unusual GPU usage. Since I didn’t deploy any monitoring programs to record historical processes and sockets, the investigation couldn’t get started.</p>
<p>On October 31, I received the same warning again. Each time when mining traffic is detected, the firewall will block the server’s outbound network. Loss of Internet will cause lots of troubles.</p>
<p>I suspected that someone may have suffered a <strong>supply chain attack</strong>, such as, downloading a Python package containing a virus, or cloning code from GitHub and running it without any check.</p>
<p>The immediate task is to identify who and which process was responsible.</p>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>While I can’t directly determine who or which process, I can block and log suspicious traffic for further investigation.</p>
<p>This job can be done by <code>iptables</code>:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">iptables -N LOGDROP                   <span class="comment"># create a new chain</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">iptables -A LOGDROP -j LOG --log-uid  <span class="comment"># log info</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">iptables -A LOGDROP -j DROP           <span class="comment"># drop packet</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">iptables -I OUTPUT 1 -p udp -m string --string <span class="string">&quot;bitcoin&quot;</span> --algo bm -j LOGDROP     <span class="comment"># match string &quot;bitcoin&quot; in udp packet</span></span></span><br></pre></td></tr></table></figure>

<p>The <code>--log-uid</code> option can enable UID recording in <code>/var/log/kern.log</code>, for example:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IN= OUT=wg0 SRC=10.1.92.3 DST=10.1.2.13 LEN=42 TOS=0x00 PREC=0x00 TTL=64 ID=23294 DF PROTO=UDP SPT=52328 DPT=2333 LEN=22 UID=2109 GID=2109</span><br></pre></td></tr></table></figure>

<h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><p>I’m waiting the next requests sent by virus.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://monsoon-cs.moe/2023-10-20-bitahub/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Monsoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023-10-20-bitahub/" class="post-title-link" itemprop="url">利用 SSH 反向隧道登录 BitaHub 中的容器并长期占用 GPU</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-10-20 00:00:00" itemprop="dateCreated datePublished" datetime="2023-10-20T00:00:00+08:00">2023-10-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-07 04:33:26" itemprop="dateModified" datetime="2024-02-07T04:33:26+08:00">2024-02-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>每年的 CVPR 前 GPU 总是供不应求，需要从其他地方借卡。USTC 有一个供校内用户使用的 <a target="_blank" rel="noopener" href="https://bitahub.ustc.edu.cn/">BitaHub</a>，但它同样有 CVPR 前一卡难求的问题，同时基于任务提交的使用模式也非常不方便，提交占用多卡的任务经常需要漫长的排队，数据管理方式更是反人类。</p>
<p>作为组里的服务器管理员，为了让自己在 CVPR 前活得轻松点，避免重蹈 2021 年 CVPR 前疲于应对资源调配的覆辙，有必要改善 BitaHub 的使用体验：</p>
<ol>
<li>如何长期占用显卡避免重复排队（虽然略不道德，但实属无奈之举）；</li>
<li>如何方便地从我们的服务器读取数据，而不是被迫使用 BitaHub 反人类的数据管理模式；</li>
<li>如何尽量使 BitaHub 的 GPU 使用体验接近组里的服务器，降低迁移成本，提高资源调度的灵活性。</li>
</ol>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>BitaHub 中的任务是以 docker 容器的方式运行的，因而给了我们在容器里配置我们想要的环境的可能，只需要通过某种方式登录 ssh 到容器中。</p>
<p>经过研究发现，只要启动命令不停止运行，BitaHub 中的容器就会长期运行，不释放 GPU 资源。<strong>同时 BitaHub 中的容器是可以联网的</strong>，而且 BitaHub 的网页上还贴心地给出了每个任务容器中 root 用户的 ssh 私钥。</p>
<p>这些给了我们利用的机会，只需要在容器内运行一个 tunnel 程序以让外部得以访问容器中的 22 端口，就能登录并长期占用资源。同时由于容器联网，也可以直接挂载校内其他服务器的文件系统。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>最终选择的 tunnel 程序是 <code>ssh</code>，它可以创建反向隧道：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -i &lt;key_file&gt; -F none -o &quot;StrictHostKeyChecking no&quot; -o &quot;ServerAliveInterval 15&quot; -v -N -R &lt;port&gt;:localhost:22 jump@&lt;jumpserver&gt;</span><br></pre></td></tr></table></figure>

<p>在 <code>jumpserver</code> 中配置用户 <code>jump</code> 并允许特定私钥登录，然后用某种方式把私钥传递进容器（可以直接打包进镜像，但我选择了更方便的方式——创建一个 BitaHub 数据集存放，每个任务添加这个数据集即可）。</p>
<p>容器的启动命令就是上述命令（考虑到网络波动，可以套一层 <code>while true</code> 循环或者用 <code>autossh</code> 自动重连），启动后就在 <code>&lt;jumpserver&gt;</code> 上的 <code>&lt;port&gt;</code> 端口创建了一个反向隧道，<code>&lt;port&gt;</code> 被映射到了容器内的 <code>22</code> 端口。</p>
<p>可以在 <code>&lt;jumpserver&gt;</code> 的 <code>sshd_config</code> 中配置 <code>GatewayPorts yes</code>，这样反向隧道就会监听 <code>0.0.0.0</code> 而不是 <code>127.0.0.1</code>。不这样做的话我就要在 <code>&lt;jumpserver&gt;</code> 上给每个人创建用户，或者每个端口用 <code>iptables</code> 转发，但这太过繁琐。绑定 <code>0.0.0.0</code> 则可以直接从现有的 VPN 网络中访问。</p>
<p>挂载文件系统的方式有很多选择，考虑到安全性和便捷性，我选择了 SSHFS。NFS 直接暴露于公网过于危险，而 NFS 用户验证的配置又过于繁琐。同时 BitaHub 运行容器的内核既没加载 <code>wireguard</code> kmod 也没映射 <code>/dev/net/tun</code>，因此无法利用 VPN 保护数据安全。SSHFS 可以直接复用现存的用户认证方式，而 SSH 流量本身也更容易被潜在的机房防火墙放过。</p>
<p>使用如下命令挂载 SSHFS：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sshfs -o reconnect,ServerAliveInterval=15,ServerAliveCountMax=30,ssh_command=&#x27;ssh -p &lt;dataserver_port&gt; -i &lt;key_file&gt;&#x27; &lt;user&gt;@&lt;dataserver&gt;:/path /path</span><br></pre></td></tr></table></figure>

<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>TODO</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://monsoon-cs.moe/2023-09-26-nginx-quic-with-ssl-preread/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Monsoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023-09-26-nginx-quic-with-ssl-preread/" class="post-title-link" itemprop="url">Nginx 启用 QUIC 并和 SNI 分流共存</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 00:00:00" itemprop="dateCreated datePublished" datetime="2023-09-26T00:00:00+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-07 04:33:26" itemprop="dateModified" datetime="2024-02-07T04:33:26+08:00">2024-02-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>Nginx 自从 1.25.0 版本以来对 QUIC 的支持<a target="_blank" rel="noopener" href="https://nginx.org/en/docs/quic.html">已被合并入 mainline</a>，对于想体验的用户而言可以直接使用官方发布的 <code>nginx</code> docker 镜像，非常方便。</p>
<p>但是我的服务器上的 nginx 使用了 <a target="_blank" rel="noopener" href="https://nginx.org/en/docs/stream/ngx_stream_ssl_preread_module.html">SNI 分流</a>，源于 <a target="_blank" rel="noopener" href="https://github.com/ihciah/shadow-tls">Shadow TLS</a> 和 <a target="_blank" rel="noopener" href="https://github.com/XTLS/REALITY">Xray Reality</a> 等新一代基于 TLS 的代理协议的需求。这些代理协议并不能由 nginx 代为处理 TLS 层（和之前可以使用 gPRC&#x2F;WebSocket 等作为数据传输方式的协议不同），但为了实现最好的伪装效果，使用 <code>443/tcp</code> 端口是有必要的（伪装的白名单目标网站一般情况下也只会在 <code>443/tcp</code> 端口开放 HTTPS 服务）。因此 <code>443/tcp</code> 端口的复用是必要的。</p>
<p>如果要让 SNI 分流和 QUIC 共存，在原来的 SNI 分流配置上只需要给每个 server 加上 <code>listen 443 quic</code> 即可。示例配置如下。</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">http</span> &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">    <span class="section">server</span> &#123;</span><br><span class="line">        <span class="attribute">server_name</span> example.com;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 443/tcp 已经被 nginx stream 占用，不能再次监听</span></span><br><span class="line">        <span class="comment"># listen 443 ssl http2 reuseport so_keepalive=on;</span></span><br><span class="line">        <span class="comment"># listen [::]:443 ssl http2 reuseport so_keepalive=on;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 监听 443/udp 端口并启用 QUIC</span></span><br><span class="line">        <span class="comment"># ref: https://nginx.org/en/docs/http/ngx_http_v3_module.html</span></span><br><span class="line">        <span class="attribute">listen</span> <span class="number">443</span> quic reuseport;</span><br><span class="line">        <span class="attribute">listen</span> [::]:<span class="number">443</span> quic reuseport;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 监听 unix domain socket 以接受 stream 传送过来的连接，也可以使用本地端口</span></span><br><span class="line">        <span class="comment"># 接受 proxy_protocol，否则 log 中显示的链接源地址都是 unix:</span></span><br><span class="line">        <span class="attribute">listen</span> unix:/dev/shm/nginx-example.sock ssl http2 proxy_protocol;</span><br><span class="line">        <span class="attribute">set_real_ip_from</span> unix:;  <span class="comment"># 只对于来自 unix domain socket 的连接覆盖其源地址</span></span><br><span class="line">        <span class="attribute">real_ip_header</span> proxy_protocol;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">add_header</span> Alt-Svc <span class="string">&#x27;h3=&quot;:443&quot;; ma=86400&#x27;</span>;  <span class="comment"># used to advertise the availability of HTTP/3</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="section">server</span> &#123;</span><br><span class="line">        <span class="attribute">server_name</span> foo.example.com;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 可以多个域名共享 443/udp</span></span><br><span class="line">        <span class="attribute">listen</span> <span class="number">443</span> quic;</span><br><span class="line">        <span class="attribute">listen</span> [::]:<span class="number">443</span> quic;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">listen</span> unix:/dev/shm/nginx-example-foo.sock ssl http2 proxy_protocol;</span><br><span class="line">        <span class="attribute">set_real_ip_from</span> unix:;</span><br><span class="line">        <span class="attribute">real_ip_header</span> proxy_protocol;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">add_header</span> Alt-Svc <span class="string">&#x27;h3=&quot;:443&quot;; ma=86400&#x27;</span>;  <span class="comment"># used to advertise the availability of HTTP/3</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="section">stream</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据 TLS SNI 分流</span></span><br><span class="line">    <span class="attribute">map</span> <span class="variable">$ssl_preread_server_name</span> <span class="variable">$name</span> &#123;</span><br><span class="line">        example.<span class="attribute">com</span>             unix:/dev/shm/nginx-example.sock;</span><br><span class="line">        foo.example.<span class="attribute">com</span>         unix:/dev/shm/nginx-example-foo.sock;</span><br><span class="line">        learn.microsoft.<span class="attribute">com</span>     <span class="number">127.0.0.1:8443</span>;  <span class="comment"># 用于 shadow-tls/xray-reality 等</span></span><br><span class="line">        <span class="attribute">default</span>                 unix:/dev/shm/nginx-default.sock;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="section">server</span> &#123;</span><br><span class="line">        <span class="comment"># 监听 443/tcp 并根据 SNI 分流</span></span><br><span class="line">        <span class="attribute">listen</span> <span class="number">443</span> reuseport so_keepalive=<span class="literal">on</span>;</span><br><span class="line">        <span class="attribute">listen</span> [::]:<span class="number">443</span> reuseport so_keepalive=<span class="literal">on</span>;</span><br><span class="line">        <span class="attribute">proxy_pass</span> <span class="variable">$name</span>;</span><br><span class="line">        <span class="attribute">ssl_preread</span> <span class="literal">on</span>;</span><br><span class="line">        <span class="attribute">proxy_protocol</span> <span class="literal">on</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>目前 <code>curl</code>&#x2F;<code>wget</code> mainline 还没有支持 QUIC，可以使用 <code>ymuski/curl-http3</code> 这个 docker 镜像：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker run -it --<span class="built_in">rm</span> ymuski/curl-http3 curl https://static.monsoon-cs.moe/public/ --http3 -IL</span></span><br><span class="line"></span><br><span class="line">HTTP/3 200</span><br><span class="line">server: nginx/1.25.2</span><br><span class="line">date: Tue, 26 Sep 2023 14:52:29 GMT</span><br><span class="line">content-type: text/html; charset=utf-8</span><br><span class="line">strict-transport-security: max-age=63072000</span><br><span class="line">alt-svc: h3=&quot;:443&quot;; ma=86400</span><br></pre></td></tr></table></figure>

<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a target="_blank" rel="noopener" href="https://nginx.org/en/docs/stream/ngx_stream_ssl_preread_module.html">https://nginx.org/en/docs/stream/ngx_stream_ssl_preread_module.html</a></li>
<li><a target="_blank" rel="noopener" href="https://nginx.org/en/docs/http/ngx_http_v3_module.html">https://nginx.org/en/docs/http/ngx_http_v3_module.html</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://monsoon-cs.moe/2023-06-19-mkl-on-amd/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Monsoon">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Monsoon's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023-06-19-mkl-on-amd/" class="post-title-link" itemprop="url">优化 MKL 在 AMD CPU 上的性能</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-19 00:00:00" itemprop="dateCreated datePublished" datetime="2023-06-19T00:00:00+08:00">2023-06-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-07 04:33:26" itemprop="dateModified" datetime="2024-02-07T04:33:26+08:00">2024-02-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>实验室有一些 AMD EPYC 7713 的服务器，采购的原因是组里有一些人的程序有非常高的 CPU 负载（我也不知道是什么负载，为什么不能跑在 GPU 上，我也没有精力去逐个帮助解决），框框多的 AMD 处理器非常适合这种需求。</p>
<p>不过 AMD 的处理器虽然香，用在炼丹实验室会有额外的问题：Anaconda 安装的 numpy 和 PyTorch 默认都使用了 MKL 作为 BLAS 的实现，MKL 的 library function 也是大部分高 CPU 负载程序的热点，但 <strong>MKL 会判断自己是否在 Intel CPU 上运行，如果不是，则没有优化效果。</strong></p>
<p>由于这是炼丹实验室，大家很少有足够的 HPC 基础去自己编译适合的 numpy 和 PyTorch 版本，也很难脱离 Anaconda，对于 MKL 的依赖因此很难去除。为此需要一个<strong>对一般用户无感知的解决方案</strong>。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>通过搜索引擎可以搜索到一个广为流传解决方案：设置环境变量 <code>MKL_DEBUG_CPU_TYPE=5</code>。这是个曾经有效的解决方案，但<strong>对于 MKL 2020 及之后的版本不再有效</strong>。</p>
<p>最终我在<a target="_blank" rel="noopener" href="https://documentation.sigma2.no/jobs/mkl.html">此处</a>找到了更巧妙的解决方案。</p>
<p>MKL 会调用一个 <code>mkl_serv_intel_cpu_true()</code> 函数以检查自己是否运行在 Intel CPU 上，只要提供一个虚假的、始终返回 <code>1</code> 的 <code>mkl_serv_intel_cpu_true()</code>，即可欺骗 MKL 让它认为自己在 Intel CPU 上运行。</p>
<p>为此，可以利用 Linux 的 <strong><code>LD_PRELOAD</code> 机制</strong>。<code>LD_PRELOAD</code> 指向的动态链接库有最高的加载优先级，只要编译一个想要的 <code>mkl_serv_intel_cpu_true()</code> 函数为 <code>so</code> 文件，并用 <code>LD_PRELOAD</code> 指向它，即可抢先完成此函数的加载。</p>
<blockquote>
<p>笔者也经常有耳闻 <code>LD_PRELOAD</code> 机制被用于库函数劫持攻击，此处算是一种妙用。</p>
</blockquote>
<h2 id="具体实施"><a href="#具体实施" class="headerlink" title="具体实施"></a>具体实施</h2><p>新建 <code>mkl_trick.c</code>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">mkl_serv_intel_cpu_true</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用 <code>gcc -shared -fPIC -o libmkl_trick.so mkl_trick.c</code> 编译，并将生成的 <code>libmkl_trick.so</code> 复制到 <code>/usr/local/lib</code>。</p>
<p>在 Shell 的全局初始化文件中加入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> MKL_DEBUG_CPU_TYPE=5  <span class="comment"># 兼容旧版本 MKL</span></span><br><span class="line"><span class="built_in">export</span> MKL_ENABLE_INSTRUCTIONS=AVX2  <span class="comment"># 可选，指明 MKL 可以使用 AVX2</span></span><br><span class="line"><span class="built_in">export</span> LD_PRELOAD=/usr/local/lib/libmkl_trick.so</span><br></pre></td></tr></table></figure>

<p>实验室的同学有的用 Bash 也有的用 ZSH，所以两者都要修改：</p>
<ul>
<li>Bash: 新建文件 <code>/etc/profile.d/mkl.sh</code> 并添加上述内容</li>
<li>ZSH: 添加到 <code>/etc/zsh/zshenv</code></li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a target="_blank" rel="noopener" href="https://documentation.sigma2.no/jobs/mkl.html">https://documentation.sigma2.no/jobs/mkl.html</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Monsoon</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/monsoon235" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.28/fancybox/fancybox.umd.js" integrity="sha256-ytMJGN3toR+a84u7g7NuHm91VIR06Q41kMWDr2pq7Zo=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
